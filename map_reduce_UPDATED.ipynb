{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4cd4f701",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "4cd4f701"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/map-reduce.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239947-lesson-3-map-reduce)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36737349-c949-4d64-9aa3-3767cbd02ad1",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "36737349-c949-4d64-9aa3-3767cbd02ad1"
      },
      "source": [
        "# Map-reduce\n",
        "\n",
        "## Review\n",
        "\n",
        "We're building up to a multi-agent research assistant that ties together all of the modules from this course.\n",
        "\n",
        "To build this multi-agent assistant, we've been introducing a few LangGraph controllability topics.\n",
        "\n",
        "We just covered parallelization and sub-graphs.\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, we're going to cover [map reduce](https://docs.langchain.com/oss/python/langgraph/use-graph-api#map-reduce-and-the-send-api)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f24e95c8",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "f24e95c8"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langchain_openai langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ff57cbf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "ff57cbf7",
        "outputId": "c5dcc06b-1f97-4364-ec15-025a024ec194"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-91616674.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{var}: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0m_set_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Open ai API key is to be used HERE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-91616674.py\u001b[0m in \u001b[0;36m_set_env\u001b[0;34m(var)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_set_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{var}: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0m_set_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Open ai API key is to be used HERE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             )\n\u001b[0;32m-> 1159\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"Open ai API key is to be used HERE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbcd868a",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "cbcd868a"
      },
      "source": [
        "We'll use [LangSmith](https://docs.langchain.com/langsmith/home) for [tracing](https://docs.langchain.com/langsmith/observability-concepts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fdc647f",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "9fdc647f"
      },
      "outputs": [],
      "source": [
        "_set_env(\"LANGSMITH key is to be used here\")\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bbe9b9f-4375-4bca-8e32-7d57cb861469",
      "metadata": {
        "id": "2bbe9b9f-4375-4bca-8e32-7d57cb861469"
      },
      "source": [
        "## Problem\n",
        "\n",
        "Map-reduce operations are essential for efficient task decomposition and parallel processing.\n",
        "\n",
        "It has two phases:\n",
        "\n",
        "(1) `Map` - Break a task into smaller sub-tasks, processing each sub-task in parallel.\n",
        "\n",
        "(2) `Reduce` - Aggregate the results across all of the completed, parallelized sub-tasks.\n",
        "\n",
        "Let's design a system that will do two things:\n",
        "\n",
        "(1) `Map` - Create a set of jokes about a topic.\n",
        "\n",
        "(2) `Reduce` - Pick the best joke from the list.\n",
        "\n",
        "We'll use an LLM to do the job generation and selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "994cf903-1ed6-4ae2-b32a-7891a2808f81",
      "metadata": {
        "id": "994cf903-1ed6-4ae2-b32a-7891a2808f81"
      },
      "outputs": [],
      "source": [
        "subjects_prompt = \"\"\"Generate a list of 3 sub-topics related to this overall theme: {topic}.\"\"\"\n",
        "quote_prompt = \"\"\"Write a short inspirational quote about {subject}.\"\"\"\n",
        "best_quote_prompt = \"\"\"Below are several quotes about {topic}. Choose the most inspiring one and return its ID (starting from 0). Quotes:\\n\\n {quotes}\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3b883cc-3469-4e96-b1a4-deadf7bf3ce5",
      "metadata": {
        "id": "f3b883cc-3469-4e96-b1a4-deadf7bf3ce5"
      },
      "source": [
        "## State\n",
        "\n",
        "### Parallelizing joke generation\n",
        "\n",
        "First, let's define the entry point of the graph that will:\n",
        "\n",
        "* Take a user input topic\n",
        "* Produce a list of joke topics from it\n",
        "* Send each joke topic to our above joke generation node\n",
        "\n",
        "Our state has a `jokes` key, which will accumulate jokes from parallelized joke generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "099218ca-ee78-4291-95a1-87ee61382e3b",
      "metadata": {
        "id": "099218ca-ee78-4291-95a1-87ee61382e3b"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel  # ✅ this import is required\n",
        "\n",
        "class Subjects(BaseModel):\n",
        "    subjects: list[str]\n",
        "\n",
        "class BestQuote(BaseModel):\n",
        "    id: int\n",
        "\n",
        "class OverallState(TypedDict):\n",
        "    topic: str\n",
        "    subjects: list\n",
        "    quotes: Annotated[list, operator.add]\n",
        "    best_selected_quote: str\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7176d1c-4a88-4b0f-a960-ee04a45279bd",
      "metadata": {
        "id": "c7176d1c-4a88-4b0f-a960-ee04a45279bd"
      },
      "source": [
        "Generate subjects for jokes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "45010efd-ad31-4daa-b77e-aaec79ef0309",
      "metadata": {
        "id": "45010efd-ad31-4daa-b77e-aaec79ef0309"
      },
      "outputs": [],
      "source": [
        "def generate_topics(state: OverallState):\n",
        "    prompt = f\"List 3 themes or ideas related to {state['topic']} that can inspire people.\"\n",
        "    response = model.with_structured_output(Subjects).invoke(prompt)\n",
        "    return {\"subjects\": response.subjects}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5296bb0-c163-4e5c-8181-1e305b37442a",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "e5296bb0-c163-4e5c-8181-1e305b37442a"
      },
      "source": [
        "Here is the magic: we use the  [Send](https://docs.langchain.com/oss/python/langgraph/graph-api/#send) to create a joke for each subject.\n",
        "\n",
        "This is very useful! It can automatically parallelize joke generation for any number of subjects.\n",
        "\n",
        "* `generate_joke`: the name of the node in the graph\n",
        "* `{\"subject\": s`}: the state to send\n",
        "\n",
        "`Send` allow you to pass any state that you want to `generate_joke`! It does not have to align with `OverallState`.\n",
        "\n",
        "In this case, `generate_joke` is using its own internal state, and we can populate this via `Send`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bc83e575-11f6-41a9-990a-adb571bcda06",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "bc83e575-11f6-41a9-990a-adb571bcda06"
      },
      "outputs": [],
      "source": [
        "from langgraph.types import Send\n",
        "def continue_to_quotes(state: OverallState):\n",
        "    return [Send(\"generate_quote\", {\"subject\": s}) for s in state[\"subjects\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9847192d-d358-411e-90c0-f06be0738717",
      "metadata": {
        "id": "9847192d-d358-411e-90c0-f06be0738717"
      },
      "source": [
        "### Joke generation (map)\n",
        "\n",
        "Now, we just define a node that will create our jokes, `generate_joke`!\n",
        "\n",
        "We write them back out to `jokes` in `OverallState`!\n",
        "\n",
        "This key has a reducer that will combine lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bcddc567-73d3-4fb3-bfc5-1bea538f2aab",
      "metadata": {
        "id": "bcddc567-73d3-4fb3-bfc5-1bea538f2aab"
      },
      "outputs": [],
      "source": [
        "class QuoteState(TypedDict):\n",
        "    subject: str\n",
        "\n",
        "class Quote(BaseModel):\n",
        "    quote: str\n",
        "\n",
        "def generate_quote(state: QuoteState):\n",
        "    prompt = f\"Write a short inspirational quote about {state['subject']}.\"\n",
        "    response = model.with_structured_output(Quote).invoke(prompt)\n",
        "    return {\"quotes\": [response.quote]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02960657-d174-4076-99a8-b3f9eea015f4",
      "metadata": {
        "id": "02960657-d174-4076-99a8-b3f9eea015f4"
      },
      "source": [
        "### Best joke selection (reduce)\n",
        "\n",
        "Now, we add logic to pick the best joke."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8d672870-75e3-4307-bda0-c41a86cbbaff",
      "metadata": {
        "id": "8d672870-75e3-4307-bda0-c41a86cbbaff"
      },
      "outputs": [],
      "source": [
        "def best_quote(state: OverallState):\n",
        "    quotes = \"\\n\\n\".join(state[\"quotes\"])\n",
        "    prompt = f\"Here are several quotes about {state['topic']}. Choose the most inspiring one and return its ID (starting from 0). Quotes:\\n\\n{quotes}\"\n",
        "    response = model.with_structured_output(BestQuote).invoke(prompt)\n",
        "    return {\"best_selected_quote\": state[\"quotes\"][response.id]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "837cd12e-5bff-426e-97f4-c774df998cfb",
      "metadata": {
        "id": "837cd12e-5bff-426e-97f4-c774df998cfb"
      },
      "source": [
        "## Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ae6be4b-144e-483c-88ad-ce86d6477a0d",
      "metadata": {
        "id": "2ae6be4b-144e-483c-88ad-ce86d6477a0d"
      },
      "outputs": [],
      "source": [
        "# ✅ Imports\n",
        "import operator\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.types import Send\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from IPython.display import Image\n",
        "import os, getpass\n",
        "\n",
        "# ✅ Set API key\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPen AI API Key is to be used here\")\n",
        "\n",
        "# ✅ Prompts\n",
        "subjects_prompt = \"\"\"Generate a list of 3 sub-topics that are all related to this overall topic: {topic}.\"\"\"\n",
        "fact_prompt = \"\"\"Generate an interesting fact about {subject}.\"\"\"\n",
        "best_fact_prompt = \"\"\"Below are a bunch of facts about {topic}. Select the most fascinating one!\n",
        "Return the ID of the best one, starting 0 as the ID for the first fact. Facts:\\n\\n{facts}\"\"\"\n",
        "summary_prompt = \"\"\"Write a short 2-line summary combining all the best points about {topic} from these facts:\\n\\n{facts}\"\"\"\n",
        "\n",
        "# ✅ LLM\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e21dc7c9-0add-4125-be76-af701adb874a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e21dc7c9-0add-4125-be76-af701adb874a",
        "outputId": "aad837aa-eeeb-4672-80d5-c96759a191ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'generate_topics': {'subjects': ['Planets', 'Stars', 'Galaxies']}}\n",
            "{'generate_fact': {'facts': [\"Fun fact about Planets: It's fascinating!\"]}}\n",
            "{'generate_fact': {'facts': [\"Fun fact about Stars: It's fascinating!\"]}}\n",
            "{'generate_fact': {'facts': [\"Fun fact about Galaxies: It's fascinating!\"]}}\n",
            "{'best_fact': {'best_selected_fact': \"Fun fact about Stars: It's fascinating!\"}}\n",
            "\n",
            "✅ Best Selected Fact:\n",
            "Fun fact about Stars: It's fascinating!\n"
          ]
        }
      ],
      "source": [
        "# ✅ MAP-REDUCE (FACTS VERSION — OFFLINE SIMULATION)\n",
        "\n",
        "# --- Imports ---\n",
        "import operator\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langgraph.types import Send\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from IPython.display import Image\n",
        "\n",
        "# --- Mock Model to Simulate LLM Behavior ---\n",
        "class MockModel:\n",
        "    def with_structured_output(self, schema):\n",
        "        # returns itself to allow chaining\n",
        "        return self\n",
        "\n",
        "    def invoke(self, prompt):\n",
        "        # Return dummy structured outputs based on the schema\n",
        "        if \"list of 3 sub-topics\" in prompt:\n",
        "            class Dummy:\n",
        "                subjects = [\"Planets\", \"Stars\", \"Galaxies\"]\n",
        "            return Dummy()\n",
        "        elif \"interesting fact about\" in prompt:\n",
        "            class Dummy:\n",
        "                fact = f\"Fun fact about {prompt.split('about ')[-1].strip('.')}: It's fascinating!\"\n",
        "            return Dummy()\n",
        "        elif \"Select the most fascinating one\" in prompt:\n",
        "            class Dummy:\n",
        "                id = 1\n",
        "            return Dummy()\n",
        "        else:\n",
        "            class Dummy:\n",
        "                text = \"Default response\"\n",
        "            return Dummy()\n",
        "\n",
        "# --- Initialize Mock Model ---\n",
        "model = MockModel()\n",
        "\n",
        "# --- Prompts (kept for learning structure) ---\n",
        "subjects_prompt = \"\"\"Generate a list of 3 sub-topics that are all related to this overall topic: {topic}.\"\"\"\n",
        "fact_prompt = \"\"\"Generate an interesting fact about {subject}.\"\"\"\n",
        "best_fact_prompt = \"\"\"Below are a bunch of facts about {topic}. Select the most fascinating one!\n",
        "Return the ID of the best one, starting 0 as the ID for the first fact. Facts:\\n\\n{facts}\"\"\"\n",
        "\n",
        "# --- Data Models (for structure) ---\n",
        "class Subjects(BaseModel):\n",
        "    subjects: list[str]\n",
        "\n",
        "class BestFact(BaseModel):\n",
        "    id: int\n",
        "\n",
        "class Fact(BaseModel):\n",
        "    fact: str\n",
        "\n",
        "# --- State Definitions ---\n",
        "class OverallState(TypedDict):\n",
        "    topic: str\n",
        "    subjects: list\n",
        "    facts: Annotated[list, operator.add]\n",
        "    best_selected_fact: str\n",
        "\n",
        "class FactState(TypedDict):\n",
        "    subject: str\n",
        "\n",
        "# --- Node 1: Generate sub-topics ---\n",
        "def generate_topics(state: OverallState):\n",
        "    prompt = subjects_prompt.format(topic=state[\"topic\"])\n",
        "    response = model.with_structured_output(Subjects).invoke(prompt)\n",
        "    return {\"subjects\": response.subjects}\n",
        "\n",
        "# --- Node 2: Continue to facts (Map phase dispatcher) ---\n",
        "def continue_to_facts(state: OverallState):\n",
        "    return [Send(\"generate_fact\", {\"subject\": s}) for s in state[\"subjects\"]]\n",
        "\n",
        "# --- Node 3: Generate facts (Map phase worker) ---\n",
        "def generate_fact(state: FactState):\n",
        "    prompt = fact_prompt.format(subject=state[\"subject\"])\n",
        "    response = model.with_structured_output(Fact).invoke(prompt)\n",
        "    return {\"facts\": [response.fact]}\n",
        "\n",
        "# --- Node 4: Select best fact (Reduce phase) ---\n",
        "def best_fact(state: OverallState):\n",
        "    facts = \"\\n\\n\".join(state[\"facts\"])\n",
        "    prompt = best_fact_prompt.format(topic=state[\"topic\"], facts=facts)\n",
        "    response = model.with_structured_output(BestFact).invoke(prompt)\n",
        "    return {\"best_selected_fact\": state[\"facts\"][response.id]}\n",
        "\n",
        "# --- Build the Graph ---\n",
        "graph = StateGraph(OverallState)\n",
        "graph.add_node(\"generate_topics\", generate_topics)\n",
        "graph.add_node(\"generate_fact\", generate_fact)\n",
        "graph.add_node(\"best_fact\", best_fact)\n",
        "graph.add_edge(START, \"generate_topics\")\n",
        "graph.add_conditional_edges(\"generate_topics\", continue_to_facts, [\"generate_fact\"])\n",
        "graph.add_edge(\"generate_fact\", \"best_fact\")\n",
        "graph.add_edge(\"best_fact\", END)\n",
        "\n",
        "# --- Compile the Graph ---\n",
        "app = graph.compile()\n",
        "Image(app.get_graph().draw_mermaid_png())\n",
        "\n",
        "# --- Execute the Graph ---\n",
        "for s in app.stream({\"topic\": \"space\"}):\n",
        "    print(s)\n",
        "\n",
        "print(\"\\n✅ Best Selected Fact:\")\n",
        "print(s[\"best_fact\"][\"best_selected_fact\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a96517e-77ab-46e2-95e2-79168c044e9c",
      "metadata": {
        "id": "2a96517e-77ab-46e2-95e2-79168c044e9c"
      },
      "source": [
        "## Studio\n",
        "\n",
        "**⚠️ Notice**\n",
        "\n",
        "Since filming these videos, we've updated Studio so that it can now be run locally and accessed through your browser. This is the preferred way to run Studio instead of using the Desktop App shown in the video. It is now called _LangSmith Studio_ instead of _LangGraph Studio_. Detailed setup instructions are available in the \"Getting Setup\" guide at the start of the course. You can find a description of Studio [here](https://docs.langchain.com/langsmith/studio), and specific details for local deployment [here](https://docs.langchain.com/langsmith/quick-start-studio#local-development-server).  \n",
        "To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
        "\n",
        "```\n",
        "langgraph dev\n",
        "```\n",
        "\n",
        "You should see the following output:\n",
        "```\n",
        "- 🚀 API: http://127.0.0.1:2024\n",
        "- 🎨 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
        "- 📚 API Docs: http://127.0.0.1:2024/docs\n",
        "```\n",
        "\n",
        "Open your browser and navigate to the **Studio UI** URL shown above.\n",
        "\n",
        "Let's load the above graph in the Studio UI, which uses `module-4/studio/map_reduce.py` set in `module-4/studio/langgraph.json`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "741a5e45-9a4c-43b4-8393-9298b3dcda53",
      "metadata": {
        "id": "741a5e45-9a4c-43b4-8393-9298b3dcda53"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}