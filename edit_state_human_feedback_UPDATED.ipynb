{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "147e576c",
      "metadata": {
        "id": "147e576c"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/edit-state-human-feedback.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239520-lesson-3-editing-state-and-human-feedback)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b2f2448-21c3-4196-9e61-0b47e7d0048b",
      "metadata": {
        "id": "3b2f2448-21c3-4196-9e61-0b47e7d0048b"
      },
      "source": [
        "# Editing graph state\n",
        "\n",
        "## Review\n",
        "\n",
        "We discussed motivations for human-in-the-loop:\n",
        "\n",
        "(1) `Approval` - We can interrupt our agent, surface state to a user, and allow the user to accept an action\n",
        "\n",
        "(2) `Debugging` - We can rewind the graph to reproduce or avoid issues\n",
        "\n",
        "(3) `Editing` - You can modify the state\n",
        "\n",
        "We showed how breakpoints support user approval, but don't yet know how to modify our graph state once our graph is interrupted!\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, let's show how to directly edit the graph state and insert human feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "95d26b8c-d958-4d21-9ca4-4636d3dfe45c",
      "metadata": {
        "id": "95d26b8c-d958-4d21-9ca4-4636d3dfe45c"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_openai langgraph_sdk langgraph-prebuilt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "d5948594",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "d5948594",
        "outputId": "e37bbfc0-44bd-41c0-a993-739e3de14ea1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2722484010.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{var}: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0m_set_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Open ai api key is to be used here \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2722484010.py\u001b[0m in \u001b[0;36m_set_env\u001b[0;34m(var)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_set_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{var}: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0m_set_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Open ai api key is to be used here \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             )\n\u001b[0;32m-> 1159\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"Open ai api key is to be used here \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65a8df1f-a76a-4803-a532-ea9802106ac8",
      "metadata": {
        "id": "65a8df1f-a76a-4803-a532-ea9802106ac8"
      },
      "source": [
        "## Editing state\n",
        "\n",
        "Previously, we introduced breakpoints.\n",
        "\n",
        "We used them to interrupt the graph and await user approval before executing the next node.\n",
        "\n",
        "But breakpoints are also [opportunities to modify the graph state](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/edit-graph-state/).\n",
        "\n",
        "Let's set up our agent with a breakpoint before the `assistant` node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "bcf24f05-ac2b-455e-846c-0c50ac86e1f4",
      "metadata": {
        "id": "bcf24f05-ac2b-455e-846c-0c50ac86e1f4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Open ai API key is to be used HERE \")\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# --- Core arithmetic functions ---\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two integers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two integers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Safely divide a by b and handle division by zero.\"\"\"\n",
        "    try:\n",
        "        return a / b\n",
        "    except ZeroDivisionError:\n",
        "        return float(\"inf\")\n",
        "\n",
        "# --- New tweak: feedback-aware operation ---\n",
        "def adjust_value(a: int, feedback: str) -> int:\n",
        "    \"\"\"Dynamically adjust a value based on human feedback text.\"\"\"\n",
        "    if \"increase\" in feedback.lower():\n",
        "        return a + 1\n",
        "    elif \"decrease\" in feedback.lower():\n",
        "        return a - 1\n",
        "    return a\n",
        "\n",
        "# --- Define tools (no new arithmetic tools added, just feedback-linked one) ---\n",
        "tools = [add, multiply, divide, adjust_value]\n",
        "\n",
        "# Bind tools to LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")  # switched to lighter variant for quicker feedback loops\n",
        "llm_with_tools = llm.bind_tools(tools)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5dfe84af-5c62-4c3f-8ed7-96b5261f0b7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "5dfe84af-5c62-4c3f-8ed7-96b5261f0b7b",
        "outputId": "5bf84cd0-a7d6-4103-de91-c830e1231381"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import MessagesState, START, StateGraph\n",
        "from langgraph.prebuilt import tools_condition, ToolNode\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# --- System setup ---\n",
        "sys_msg = SystemMessage(content=\"You are an interactive assistant that performs arithmetic and adapts based on user feedback.\")\n",
        "\n",
        "# --- Node definition ---\n",
        "def assistant(state: MessagesState):\n",
        "    \"\"\"Invoke LLM with tool awareness and handle dynamic state changes.\"\"\"\n",
        "    response = llm_with_tools.invoke([sys_msg] + state[\"messages\"])\n",
        "\n",
        "    #  New tweak: if feedback exists, modify the latest state before returning\n",
        "    if state.get(\"feedback\"):\n",
        "        state[\"messages\"].append(HumanMessage(content=f\"Feedback noted: {state['feedback']}\"))\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# --- Graph definition ---\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "# Define functional nodes\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# --- Control flow edges ---\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\"assistant\", tools_condition)\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "\n",
        "# --- New tweak: introduce custom breakpoint for direct human-in-the-loop edit ---\n",
        "# This allows pausing before 'tools' node to inspect or modify graph state\n",
        "memory = MemorySaver()\n",
        "graph = builder.compile(interrupt_before=[\"tools\"], checkpointer=memory)\n",
        "\n",
        "# --- Visualization ---\n",
        "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a47fd5-1f60-41dc-9206-698ed8ece530",
      "metadata": {
        "id": "92a47fd5-1f60-41dc-9206-698ed8ece530"
      },
      "source": [
        "Let's run!\n",
        "\n",
        "We can see the graph is interrupted before the chat model responds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a2ce488d-00e4-492e-a62c-dd98702c313f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2ce488d-00e4-492e-a62c-dd98702c313f",
        "outputId": "3dd7bfe5-3d45-4a45-ae60-12c27bbeec17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3\n",
            "Provide feedback (e.g., 'increase precision' or 'simplify result'): increase precision\n",
            "🪄 Injected feedback into graph state: increase precision\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Result: 6 (mocked multiplication output).\n"
          ]
        }
      ],
      "source": [
        "# --- Mock setup for offline / no-API testing ---\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "class MockLLM:\n",
        "    \"\"\"Simulates an LLM that performs basic arithmetic reasoning offline.\"\"\"\n",
        "    def invoke(self, messages):\n",
        "        last = messages[-1].content.lower()\n",
        "        if \"multiply\" in last:\n",
        "            return AIMessage(content=\"Result: 6 (mocked multiplication output).\")\n",
        "        elif \"add\" in last:\n",
        "            return AIMessage(content=\"Result: 5 (mocked addition output).\")\n",
        "        elif \"divide\" in last:\n",
        "            return AIMessage(content=\"Result: 2.0 (mocked division output).\")\n",
        "        else:\n",
        "            return AIMessage(content=\"Mocked assistant reply (no tool detected).\")\n",
        "\n",
        "# Replace the real LLM with our mock one\n",
        "llm_with_tools = MockLLM()\n",
        "\n",
        "# --- Input and thread configuration ---\n",
        "initial_input = {\"messages\": [HumanMessage(content=\"Multiply 2 and 3\")]}\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# --- Run graph with human-in-the-loop interaction (offline-safe) ---\n",
        "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "    #  Simulate human feedback injection into the graph state\n",
        "    if \"Multiply\" in event[\"messages\"][-1].content:\n",
        "        feedback = input(\"Provide feedback (e.g., 'increase precision' or 'simplify result'): \")\n",
        "        event[\"feedback\"] = feedback\n",
        "        print(f\"🪄 Injected feedback into graph state: {feedback}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be478ef-bd60-4d32-8a05-5f56c93a8396",
      "metadata": {
        "id": "4be478ef-bd60-4d32-8a05-5f56c93a8396",
        "outputId": "d9da80ad-05c8-4ceb-efa9-491752eb48c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content='Multiply 2 and 3', id='e7edcaba-bfed-4113-a85b-25cc39d6b5a7')]}, next=('assistant',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a412-5b2d-601a-8000-4af760ea1d0d'}}, metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}}, created_at='2024-09-03T22:09:10.966883+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a412-5b28-6ace-bfff-55d7a2c719ae'}}, tasks=(PregelTask(id='dbee122a-db69-51a7-b05b-a21fab160696', name='assistant', error=None, interrupts=(), state=None),))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state = graph.get_state(thread)\n",
        "state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36ef63a1-2ab8-416d-babf-d35054e294f0",
      "metadata": {
        "id": "36ef63a1-2ab8-416d-babf-d35054e294f0"
      },
      "source": [
        "Now, we can directly apply a state update.\n",
        "\n",
        "Remember, updates to the `messages` key will use the `add_messages` reducer:\n",
        "\n",
        "* If we want to over-write the existing message, we can supply the message `id`.\n",
        "* If we simply want to append to our list of messages, then we can pass a message without an `id` specified, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9179cff1-e529-473a-9ce2-e23b932c2063",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9179cff1-e529-473a-9ce2-e23b932c2063",
        "outputId": "39359ea6-35a6-4839-d6a1-2c3e15f389b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Graph state updated successfully with new human feedback and input correction.\n"
          ]
        }
      ],
      "source": [
        "# --- Directly update graph state to simulate human correction ---\n",
        "graph.update_state(\n",
        "    thread,\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"No, actually multiply 3 and 3!\")],\n",
        "        \"feedback\": \"User corrected input after reviewing the result.\"\n",
        "    },\n",
        ")\n",
        "\n",
        "print(\"✅ Graph state updated successfully with new human feedback and input correction.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d77b8d6a-8c7b-4f7a-b723-121af25ac829",
      "metadata": {
        "id": "d77b8d6a-8c7b-4f7a-b723-121af25ac829"
      },
      "source": [
        "Let's have a look.\n",
        "\n",
        "We called `update_state` with a new message.\n",
        "\n",
        "The `add_messages` reducer appends it to our state key, `messages`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "141b6aab-ec6d-44f3-beb1-6c22ac5f2158",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "141b6aab-ec6d-44f3-beb1-6c22ac5f2158",
        "outputId": "b4cc76da-a8f6-4e00-f698-ff87d028dd3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧩 Current Graph State (after manual update and feedback):\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Result: 6 (mocked multiplication output).\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "No, actually multiply 3 and 3!\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "No, actually multiply 3 and 3!\n"
          ]
        }
      ],
      "source": [
        "# --- Retrieve and inspect the latest updated graph state ---\n",
        "new_state = graph.get_state(thread).values\n",
        "\n",
        "print(\"🧩 Current Graph State (after manual update and feedback):\")\n",
        "for msg in new_state[\"messages\"]:\n",
        "    msg.pretty_print()\n",
        "\n",
        "#  Optional: show feedback key to confirm human intervention was stored\n",
        "if \"feedback\" in new_state:\n",
        "    print(f\"\\n💬 Stored Human Feedback: {new_state['feedback']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4041959-cc3a-4168-8cf7-06d1711921d8",
      "metadata": {
        "id": "e4041959-cc3a-4168-8cf7-06d1711921d8"
      },
      "source": [
        "Now, let's proceed with our agent, simply by passing `None` and allowing it proceed from the current state.\n",
        "\n",
        "We emit the current and then proceed to execute the remaining nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f166bed2-87c9-41ec-b235-0305721c2d6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f166bed2-87c9-41ec-b235-0305721c2d6b",
        "outputId": "b78894f5-bcfb-43f6-e7c2-b9874a63547e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Resuming graph execution from the updated state...\n",
            "\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "No, actually multiply 3 and 3!\n"
          ]
        }
      ],
      "source": [
        "# --- Resume the graph from the manually updated state ---\n",
        "print(\" Resuming graph execution from the updated state...\\n\")\n",
        "\n",
        "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "    # 🧠 New tweak: confirm feedback influence\n",
        "    if \"feedback\" in event:\n",
        "        print(f\"\\n💬 Active Feedback Context: {event['feedback']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b18dc1ca",
      "metadata": {
        "id": "b18dc1ca"
      },
      "source": [
        "Now, we're back at the `assistant`, which has our `breakpoint`.\n",
        "\n",
        "We can again pass `None` to proceed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f5952731-0170-4589-a399-ee787df35400",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5952731-0170-4589-a399-ee787df35400",
        "outputId": "da3e76f7-a193-4bfa-ca84-a31c0858e35a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming the graph from the updated state...\n",
            "\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "No, actually multiply 3 and 3!\n"
          ]
        }
      ],
      "source": [
        "#  Continue execution from the current graph state\n",
        "print(\"Resuming the graph from the updated state...\\n\")\n",
        "\n",
        "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "    # 💡 Added tweak: Log which node is being executed for better traceability\n",
        "    if \"next\" in event:\n",
        "        print(f\"\\n[Debug] Next node scheduled → {event['next']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc22c3e9-b00c-4ead-b752-a682b45b3718",
      "metadata": {
        "id": "bc22c3e9-b00c-4ead-b752-a682b45b3718"
      },
      "source": [
        "### Editing graph state in Studio\n",
        "\n",
        "**⚠️ Notice**\n",
        "\n",
        "Since filming these videos, we've updated Studio so that it can now be run locally and accessed through your browser. This is the preferred way to run Studio instead of using the Desktop App shown in the video. It is now called _LangSmith Studio_ instead of _LangGraph Studio_. Detailed setup instructions are available in the \"Getting Setup\" guide at the start of the course. You can find a description of Studio [here](https://docs.langchain.com/langsmith/studio), and specific details for local deployment [here](https://docs.langchain.com/langsmith/quick-start-studio#local-development-server).  \n",
        "To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
        "\n",
        "```\n",
        "langgraph dev\n",
        "```\n",
        "\n",
        "You should see the following output:\n",
        "```\n",
        "- 🚀 API: http://127.0.0.1:2024\n",
        "- 🎨 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
        "- 📚 API Docs: http://127.0.0.1:2024/docs\n",
        "```\n",
        "\n",
        "Open your browser and navigate to the **Studio UI** URL shown above.\n",
        "\n",
        "The LangGraph API [supports editing graph state](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_edit_state/#initial-invocation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "642aabab-f822-4917-9d66-3314ac5008fd",
      "metadata": {
        "id": "642aabab-f822-4917-9d66-3314ac5008fd"
      },
      "outputs": [],
      "source": [
        "# This is the URL of the local development server\n",
        "from langgraph_sdk import get_client\n",
        "client = get_client(url=\"http://127.0.0.1:2024\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be74cb09",
      "metadata": {
        "id": "be74cb09"
      },
      "source": [
        "Our agent is defined in `studio/agent.py`.\n",
        "\n",
        "If you look at the code, you'll see that it *does not* have a breakpoint!\n",
        "\n",
        "Of course, we can add it to `agent.py`, but one very nice feature of the API is that we can pass in a breakpoint!\n",
        "\n",
        "Here, we pass a `interrupt_before=[\"assistant\"]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1c352f9e-6a0f-4a94-a083-b85b0233efa9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c352f9e-6a0f-4a94-a083-b85b0233efa9",
        "outputId": "94fe6f24-8add-421e-f172-e46f68ab54c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📨 Event Type: metadata\n",
            " No new message detected at this event.\n",
            "------------------------------------------------------------\n",
            "📨 Event Type: values\n",
            " Latest State Message → Multiply 2 and 3\n",
            "------------------------------------------------------------\n",
            "📨 Event Type: interrupt\n",
            " Latest State Message → Breakpoint before human_feedback\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#  Initialize input and create mock async thread for interaction\n",
        "initial_input = {\"messages\": \"Multiply 2 and 3\"}\n",
        "thread = {\"thread_id\": \"mock-thread-01\"}  #  replaced client.threads.create() to avoid connection\n",
        "\n",
        "#  Simulated async stream (offline-safe)\n",
        "import asyncio\n",
        "\n",
        "async def mock_stream_simulator(input_data, interrupt_before):\n",
        "    \"\"\"Simulate event stream for human-in-the-loop learning without API calls.\"\"\"\n",
        "    events = [\n",
        "        {\"event\": \"metadata\", \"messages\": []},\n",
        "        {\"event\": \"values\", \"messages\": [{\"content\": input_data[\"messages\"]}]},\n",
        "        {\"event\": \"interrupt\", \"messages\": [{\"content\": f\"Breakpoint before {interrupt_before[0]}\"}]},\n",
        "    ]\n",
        "    for e in events:\n",
        "        await asyncio.sleep(0.5)\n",
        "        yield e\n",
        "\n",
        "#  Run simulated event loop\n",
        "async for chunk in mock_stream_simulator(initial_input, [\"human_feedback\"]):\n",
        "    print(f\" Event Type: {chunk['event']}\")\n",
        "    msgs = chunk.get(\"messages\", [])\n",
        "    if msgs:\n",
        "        print(\" Latest State Message →\", msgs[-1][\"content\"])\n",
        "    else:\n",
        "        print(\" No new message detected at this event.\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13065dd9-5f43-47d6-ac2a-9dc15c0c54e6",
      "metadata": {
        "id": "13065dd9-5f43-47d6-ac2a-9dc15c0c54e6"
      },
      "source": [
        "We can get the current state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4da2c464-3e71-496a-badc-671aeee168b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4da2c464-3e71-496a-badc-671aeee168b6",
        "outputId": "b463b600-95c6-4fd8-854b-f7458b3beaf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Current Graph State Snapshot:\n",
            "{'thread_id': 'mock-thread-01', 'state': {'messages': [{'role': 'user', 'content': 'Multiply 2 and 3'}, {'role': 'assistant', 'content': 'The result is 6.'}], 'feedback': None}, 'status': 'active'}\n"
          ]
        }
      ],
      "source": [
        "# Simulated way to inspect current thread state without any API connection\n",
        "\n",
        "# Instead of making a client call, we’ll just hold the simulated state in a local dict\n",
        "current_state = {\n",
        "    \"thread_id\": thread[\"thread_id\"],\n",
        "    \"state\": {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": \"Multiply 2 and 3\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"The result is 6.\"},\n",
        "        ],\n",
        "        \"feedback\": None,\n",
        "    },\n",
        "    \"status\": \"active\"\n",
        "}\n",
        "\n",
        "# Display the simulated current state\n",
        "print(\" Current Graph State Snapshot:\")\n",
        "print(current_state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4527bbf1-0927-41a6-aeef-d15e32bbbdc3",
      "metadata": {
        "id": "4527bbf1-0927-41a6-aeef-d15e32bbbdc3"
      },
      "source": [
        "We can look at the last message in state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "801ae2d9-0551-46b8-aee2-82293cee4011",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "801ae2d9-0551-46b8-aee2-82293cee4011",
        "outputId": "a0ce4edb-9d5a-41a5-abfe-98eb26e7f4cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last Message in Current State:\n",
            "{'role': 'assistant', 'content': 'The result is 6.'}\n"
          ]
        }
      ],
      "source": [
        "# Extract the last message safely from simulated state\n",
        "last_message = current_state['state']['messages'][-1]\n",
        "\n",
        "# Display the message\n",
        "print(\"Last Message in Current State:\")\n",
        "print(last_message)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0581ba8-db3d-474d-9042-b1c7f3461caf",
      "metadata": {
        "id": "f0581ba8-db3d-474d-9042-b1c7f3461caf"
      },
      "source": [
        "We can edit it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "86b12be7-7e4a-40d0-8521-dced7c393c71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86b12be7-7e4a-40d0-8521-dced7c393c71",
        "outputId": "57e04377-da9c-4ce5-ee5f-25608b3ace1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'role': 'assistant', 'content': 'The result is 6.'}\n"
          ]
        }
      ],
      "source": [
        "# Apply the modified message back to current_state\n",
        "current_state['state']['messages'][-1] = last_message\n",
        "\n",
        "# Optional: verify the change\n",
        "print(current_state['state']['messages'][-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f84f2c24-f281-4591-90e5-de3a5547c9da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f84f2c24-f281-4591-90e5-de3a5547c9da",
        "outputId": "df6364a8-4363-4cf5-e4bf-497bb6dbc138"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'human', 'content': 'No, actually multiply 3 and 3!'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "{\n",
        "    \"type\": \"human\",\n",
        "    \"content\": \"No, actually multiply 3 and 3!\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce7b4280-6ae7-4246-9c87-44e0daa6c654",
      "metadata": {
        "id": "ce7b4280-6ae7-4246-9c87-44e0daa6c654"
      },
      "source": [
        "Remember, as we said before, updates to the `messages` key will use the same `add_messages` reducer.\n",
        "\n",
        "If we want to over-write the existing message, then we can supply the message `id`.\n",
        "\n",
        "Here, we did that. We only modified the message `content`, as shown above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "84d33b6e-32ff-4eca-8114-345e508f3481",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84d33b6e-32ff-4eca-8114-345e508f3481",
        "outputId": "b029c735-1b5f-4b89-f7a6-d3e0c2c812bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '1',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1f0b3427-1145-6bd5-800c-a2dbdb4619d5'}}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "graph.update_state(\n",
        "    {\"configurable\": {\"thread_id\": \"1\"}},\n",
        "    {\"messages\": [last_message]}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f07f0d1-7083-4827-babd-d3702eb59a37",
      "metadata": {
        "id": "1f07f0d1-7083-4827-babd-d3702eb59a37"
      },
      "source": [
        "Now, we resume by passing `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ef18d12d-e0a6-487a-9f32-ad30e2634a20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef18d12d-e0a6-487a-9f32-ad30e2634a20",
        "outputId": "589e3727-7b43-4cc5-c4a7-a089cf168a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Event Type: <class 'dict'>\n",
            "Latest Message: The result is 6.\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#  Offline simulation of streaming events from the graph — no API calls, fully local\n",
        "\n",
        "# Use the same thread_id structure expected by LangGraph\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Continue streaming from the updated state\n",
        "for event in graph.stream(\n",
        "    None,  # no new input since we're resuming from last updated state\n",
        "    thread,\n",
        "    stream_mode=\"values\",\n",
        "    interrupt_before=[\"assistant\"],  # breakpoint before assistant node to inspect flow\n",
        "):\n",
        "    print(f\" Event Type: {type(event)}\")\n",
        "\n",
        "    # Safely extract latest messages from each event\n",
        "    if \"messages\" in event and event[\"messages\"]:\n",
        "        latest_message = event[\"messages\"][-1]\n",
        "        print(\"Latest Message:\", latest_message.content)\n",
        "    else:\n",
        "        print(\" No new messages in this event.\")\n",
        "\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a82dd35-cbc8-486d-8e20-10d0c4d138d6",
      "metadata": {
        "id": "6a82dd35-cbc8-486d-8e20-10d0c4d138d6"
      },
      "source": [
        "We get the result of the tool call as `9`, as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "1d1bb3c7-dc26-4c32-b3df-865f41ef3c73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d1bb3c7-dc26-4c32-b3df-865f41ef3c73",
        "outputId": "ffcae1ca-7130-48e3-f20b-7e6b5ca6cca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Streaming graph events locally...\n",
            "\n",
            " Event 1: dict\n",
            " Message: The result is 6.\n",
            "------------------------------------------------------------\n",
            "✅ Finished local event streaming.\n"
          ]
        }
      ],
      "source": [
        "#  Offline simulation of client.runs.stream()\n",
        "# Works fully locally using LangGraph — no API key or network call required.\n",
        "\n",
        "from time import sleep\n",
        "\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "print(\" Streaming graph events locally...\\n\")\n",
        "\n",
        "# Simulate async streaming by iterating events from local graph\n",
        "for i, event in enumerate(graph.stream(\n",
        "    None,                     # no new input; resumes from updated state\n",
        "    thread,\n",
        "    stream_mode=\"values\",     # same as original\n",
        "    interrupt_before=[\"assistant\"],  # breakpoint before assistant node\n",
        ")):\n",
        "    print(f\" Event {i+1}: {type(event).__name__}\")\n",
        "\n",
        "    # Extract and print the latest message (if available)\n",
        "    if \"messages\" in event and event[\"messages\"]:\n",
        "        latest_msg = event[\"messages\"][-1]\n",
        "        print(\" Message:\", latest_msg.content)\n",
        "    else:\n",
        "        print(\" No message in this event.\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    sleep(0.4)  # small delay to mimic streaming feel\n",
        "\n",
        "print(\"✅ Finished local event streaming.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6914c5ca-27e4-421c-835a-9e4327dac12f",
      "metadata": {
        "id": "6914c5ca-27e4-421c-835a-9e4327dac12f"
      },
      "source": [
        "## Awaiting user input\n",
        "\n",
        "So, it's clear that we can edit our agent state after a breakpoint.\n",
        "\n",
        "Now, what if we want to allow for human feedback to perform this state update?\n",
        "\n",
        "We'll add a node that [serves as a placeholder for human feedback](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/wait-user-input/#setup) within our agent.\n",
        "\n",
        "This `human_feedback` node allow the user to add feedback directly to state.\n",
        "\n",
        "We specify the breakpoint using `interrupt_before` our `human_feedback` node.\n",
        "\n",
        "We set up a checkpointer to save the state of the graph up until this node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "e4b475ff-681f-4660-80dd-d6ade7bd48e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4b475ff-681f-4660-80dd-d6ade7bd48e3",
        "outputId": "da92864c-4d48-4373-86fd-395747fb7e1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7927ff3b5100>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "from langgraph.prebuilt import tools_condition as base_condition\n",
        "\n",
        "def custom_tools_condition(state):\n",
        "    result = base_condition(state)\n",
        "    return \"toolset\" if result == \"tools\" else result\n",
        "\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant_node\",\n",
        "    custom_tools_condition,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32d4ceb6-a224-4307-8196-3f53d367df5c",
      "metadata": {
        "id": "32d4ceb6-a224-4307-8196-3f53d367df5c"
      },
      "source": [
        "We will get feedback from the user.\n",
        "\n",
        "We use `.update_state` to update the state of the graph with the human response we get, as before.\n",
        "\n",
        "We use the `as_node=\"human_feedback\"` parameter to apply this state update as the specified node, `human_feedback`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "3fc7bcd6-660c-4a8a-ad8d-e6698dcf6201",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fc7bcd6-660c-4a8a-ad8d-e6698dcf6201",
        "outputId": "adca9cbb-23a9-487e-8157-05f527bf85ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Streaming Assistant Output ---\n",
            "\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3\n",
            "\n",
            "--- Human Feedback Phase ---\n",
            "\n",
            "Enter your feedback or correction: correction\n",
            "\n",
            "--- Continuing Execution ---\n",
            "\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "correction\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Mock response: Hello!\n"
          ]
        }
      ],
      "source": [
        "# --- Imports ---\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langgraph.graph.message import MessagesState\n",
        "from IPython.display import Image\n",
        "\n",
        "# --- Mock LLM function (offline-safe) ---\n",
        "def mock_llm_with_tools(messages):\n",
        "    \"\"\"Simple fake LLM to simulate behavior without API calls.\"\"\"\n",
        "    user_content = messages[-1].content.lower()\n",
        "    if \"multiply\" in user_content:\n",
        "        try:\n",
        "            nums = [int(x) for x in user_content.split() if x.isdigit()]\n",
        "            result = nums[0] * nums[1]\n",
        "            return AIMessage(content=f\"Result: {result} (mocked multiplication output).\")\n",
        "        except Exception:\n",
        "            return AIMessage(content=\"Error: unable to multiply.\")\n",
        "    return AIMessage(content=\"Mock response: Hello!\")\n",
        "\n",
        "# --- Nodes ---\n",
        "def human_feedback(state: MessagesState):\n",
        "    \"\"\"Pause for human feedback\"\"\"\n",
        "    return state\n",
        "\n",
        "def assistant(state: MessagesState):\n",
        "    \"\"\"Simulated assistant node\"\"\"\n",
        "    sys_msg = SystemMessage(content=\"You are a helpful assistant.\")\n",
        "    return {\"messages\": [mock_llm_with_tools([sys_msg] + state[\"messages\"])]}\n",
        "\n",
        "# --- Build Graph ---\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "# Add nodes\n",
        "builder.add_node(\"human_feedback\", human_feedback)\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "\n",
        "# Add edges\n",
        "builder.add_edge(START, \"human_feedback\")\n",
        "builder.add_edge(\"human_feedback\", \"assistant\")\n",
        "builder.add_edge(\"assistant\", END)\n",
        "\n",
        "# Memory checkpointer\n",
        "memory = MemorySaver()\n",
        "\n",
        "# Compile graph with interrupt point\n",
        "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
        "\n",
        "# --- Run Interaction ---\n",
        "initial_input = {\"messages\": [HumanMessage(content=\"Multiply 2 and 3\")]}\n",
        "thread = {\"configurable\": {\"thread_id\": \"demo-thread\"}}\n",
        "\n",
        "print(\"\\n--- Streaming Assistant Output ---\\n\")\n",
        "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "print(\"\\n--- Human Feedback Phase ---\\n\")\n",
        "user_input = input(\"Enter your feedback or correction: \")\n",
        "\n",
        "# ✅ Now 'human_feedback' exists in the graph\n",
        "graph.update_state(\n",
        "    thread,\n",
        "    {\"messages\": [HumanMessage(content=user_input)]},\n",
        "    as_node=\"human_feedback\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- Continuing Execution ---\\n\")\n",
        "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "abf4cf5f-c0cb-4fdb-be6b-271ae4e967e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "abf4cf5f-c0cb-4fdb-be6b-271ae4e967e2",
        "outputId": "7429612c-fba7-4d94-e782-09600ddc1dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 🔄 Starting Smart Human-in-the-Loop Execution ---\n",
            "\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "🧮 Computed Output → 6\n",
            "💡 Quick Note: Verify the arithmetic step.\n",
            "\n",
            "--- 🧠 Human Feedback Phase (Round 1) ---\n",
            "💬 Injected Feedback → Add a short explanation to the result.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add a short explanation to the result.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "💡 Mock response: Here's something insightful!\n",
            "\n",
            "--- 🧠 Human Feedback Phase (Round 2) ---\n",
            "💬 Injected Feedback → Looks good! Proceed with next query.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Looks good! Proceed with next query.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "✨ Perfect! Seems like the model has reached an optimal output.\n",
            "\n",
            "--- 🧠 Human Feedback Phase (Round 3) ---\n",
            "💬 Injected Feedback → Try using simpler language.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Try using simpler language.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "💡 Mock response: Here's something insightful!\n",
            "\n",
            "--- 🧠 Human Feedback Phase (Round 4) ---\n",
            "💬 Injected Feedback → Improve accuracy in calculation.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Improve accuracy in calculation.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "💡 Mock response: Here's something insightful!\n",
            "\n",
            "--- 🧠 Human Feedback Phase (Round 5) ---\n",
            "💬 Injected Feedback → Add a short explanation to the result.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add a short explanation to the result.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "💡 Mock response: Here's something insightful!\n",
            "\n",
            "--- 🧠 Human Feedback Phase (Round 6) ---\n",
            "✅ Feedback cycle limit reached — stabilizing output.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Looks good! Final confirmation.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "✨ Perfect! Seems like the model has reached an optimal output.\n",
            "\n",
            "--- 🧠 Human Feedback Phase (Round 7) ---\n",
            "✅ Feedback cycle limit reached — stabilizing output.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Looks good! Final confirmation.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "✨ Perfect! Seems like the model has reached an optimal output.\n",
            "\n",
            "--- 🧠 Human Feedback Phase (Round 8) ---\n",
            "✅ Feedback cycle limit reached — stabilizing output.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Looks good! Final confirmation.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "✨ Perfect! Seems like the model has reached an optimal output.\n",
            "\n",
            "--- 🧠 Human Feedback Phase (Round 9) ---\n",
            "✅ Feedback cycle limit reached — stabilizing output.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Looks good! Final confirmation.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "✨ Perfect! Seems like the model has reached an optimal output.\n",
            "\n",
            "--- 🧠 Human Feedback Phase (Round 10) ---\n",
            "✅ Feedback cycle limit reached — stabilizing output.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Looks good! Final confirmation.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "✨ Perfect! Seems like the model has reached an optimal output.\n",
            "\n",
            "--- 🧠 Human Feedback Phase (Round 11) ---\n",
            "✅ Feedback cycle limit reached — stabilizing output.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Looks good! Final confirmation.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "✨ Perfect! Seems like the model has reached an optimal output.\n",
            "\n",
            "--- 🧠 Human Feedback Phase (Round 12) ---\n",
            "✅ Feedback cycle limit reached — stabilizing output.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Looks good! Final confirmation.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "✨ Perfect! Seems like the model has reached an optimal output.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "GraphRecursionError",
          "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1236685922.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- 🔄 Starting Smart Human-in-the-Loop Execution ---\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2705\u001b[0m                     \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRAPH_RECURSION_LIMIT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2706\u001b[0m                 )\n\u001b[0;32m-> 2707\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mGraphRecursionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2708\u001b[0m             \u001b[0;31m# set final channel values as run output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2709\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 💫 FINAL SMART HUMAN-IN-THE-LOOP GRAPH (Self-Terminating)\n",
        "# ============================================================\n",
        "\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "import random, time\n",
        "\n",
        "# -------------------------\n",
        "# 🧩 1. Assistant Node\n",
        "# -------------------------\n",
        "def assistant_node(state: MessagesState):\n",
        "    \"\"\"Simulates AI assistant response to human or system feedback.\"\"\"\n",
        "    last_message = state[\"messages\"][-1].content.lower()\n",
        "\n",
        "    if \"multiply\" in last_message:\n",
        "        nums = [int(s) for s in last_message.split() if s.isdigit()]\n",
        "        result = nums[0] * nums[1] if len(nums) >= 2 else \"N/A\"\n",
        "        reply = f\"🧮 Computed Output → {result}\\n💡 Quick Note: Verified with arithmetic logic.\"\n",
        "    elif \"good\" in last_message or \"final\" in last_message:\n",
        "        reply = \"✨ Perfect! Seems like the model has reached an optimal output.\"\n",
        "    else:\n",
        "        reply = \"💡 Mock response: Here's something insightful!\"\n",
        "\n",
        "    time.sleep(0.3)\n",
        "    return {\"messages\": [AIMessage(content=reply)]}\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 🧠 2. Feedback Node\n",
        "# -------------------------\n",
        "def feedback_node(state: MessagesState):\n",
        "    \"\"\"Simulates human feedback — now stops gracefully once satisfied.\"\"\"\n",
        "    feedback_bank = [\n",
        "        \"Try using simpler language.\",\n",
        "        \"Improve accuracy in calculation.\",\n",
        "        \"Add a short explanation to the result.\",\n",
        "        \"Looks good! Proceed with next query.\",\n",
        "        \"Make the tone more conversational.\",\n",
        "    ]\n",
        "\n",
        "    feedback_round = len([m for m in state[\"messages\"] if isinstance(m, HumanMessage)]) - 1\n",
        "    print(f\"\\n--- 🧠 Human Feedback Phase (Round {feedback_round + 1}) ---\")\n",
        "\n",
        "    # 🛑 Stop condition\n",
        "    if feedback_round >= 5 or \"good\" in state[\"messages\"][-1].content.lower():\n",
        "        print(\"✅ Feedback stabilized — stopping further iterations.\")\n",
        "        return {\"messages\": [HumanMessage(content=\"Looks good! Final confirmation.\")]}\n",
        "\n",
        "    feedback = random.choice(feedback_bank)\n",
        "    print(f\"💬 Injected Feedback → {feedback}\")\n",
        "    time.sleep(0.2)\n",
        "    return {\"messages\": [HumanMessage(content=feedback)]}\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 🏗️ 3. Build the Graph\n",
        "# -------------------------\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "builder.add_node(\"assistant_node\", assistant_node)\n",
        "builder.add_node(\"feedback_node\", feedback_node)\n",
        "\n",
        "builder.add_edge(START, \"assistant_node\")\n",
        "builder.add_edge(\"assistant_node\", \"feedback_node\")\n",
        "builder.add_edge(\"feedback_node\", \"assistant_node\")\n",
        "\n",
        "# ✅ Conditional edge ensures graceful exit\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant_node\",\n",
        "    lambda state: (\n",
        "        \"true\" if \"good\" in state[\"messages\"][-1].content.lower() else \"false\"\n",
        "    ),\n",
        "    {\"true\": END, \"false\": \"feedback_node\"},\n",
        ")\n",
        "\n",
        "# Attach memory for persistent state tracking\n",
        "memory = MemorySaver()\n",
        "\n",
        "# ✅ Increase recursion limit & compile safely\n",
        "graph = builder.compile(checkpointer=memory, recursion_limit=10)\n",
        "\n",
        "# -------------------------\n",
        "# 🚀 4. Run the System\n",
        "# -------------------------\n",
        "initial_input = {\"messages\": [HumanMessage(content=\"Multiply 2 and 3\")]}\n",
        "thread = {\"configurable\": {\"thread_id\": \"loop_01\"}}\n",
        "\n",
        "print(\"\\n--- 🔄 Starting Final Smart Human-in-the-Loop Execution ---\\n\")\n",
        "\n",
        "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "print(\"\\n✅ Execution completed — model stabilized with human feedback.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}