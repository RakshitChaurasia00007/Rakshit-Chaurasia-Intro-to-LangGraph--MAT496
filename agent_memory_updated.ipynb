{
  "cells": [
    {
      "metadata": {
        "id": "83e69e148521fe6a"
      },
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-1/agent-memory.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239417-lesson-7-agent-with-memory)"
      ],
      "id": "83e69e148521fe6a"
    },
    {
      "metadata": {
        "id": "edabb1cf015fcf88"
      },
      "cell_type": "markdown",
      "source": [
        "# Agent memory\n",
        "\n",
        "## Review\n",
        "\n",
        "Previously, we built an agent that can:\n",
        "\n",
        "* `act` - let the model call specific tools\n",
        "* `observe` - pass the tool output back to the model\n",
        "* `reason` - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)\n",
        "\n",
        "![Screenshot 2024-08-21 at 12.45.32 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab7453080e6802cd1703_agent-memory1.png)\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, we're going extend our agent by introducing memory."
      ],
      "id": "edabb1cf015fcf88"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-16T16:32:25.426550Z",
          "start_time": "2025-10-16T16:32:23.545293Z"
        },
        "id": "f796d3f589b7081",
        "outputId": "3a7ac86c-3419-4a42-8026-e4f4dc7096e3"
      },
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_openai langchain_core langgraph langgraph-prebuilt"
      ],
      "id": "f796d3f589b7081",
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-16T16:32:25.435990Z",
          "start_time": "2025-10-16T16:32:25.430858Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41aff4846eb35385",
        "outputId": "b465d4cb-6790-4258-daa5-c5b68fafb7af"
      },
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"api key here\")"
      ],
      "id": "41aff4846eb35385",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "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: 路路路路路路路路路路\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "de2339c3039fd686"
      },
      "cell_type": "markdown",
      "source": [
        "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
      ],
      "id": "de2339c3039fd686"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-16T16:32:25.449930Z",
          "start_time": "2025-10-16T16:32:25.445354Z"
        },
        "id": "715576d7bdce0939"
      },
      "cell_type": "code",
      "source": [
        "_set_env(\"LANGSMITH_API_KEY\")\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
      ],
      "id": "715576d7bdce0939",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "105e5ebc78da7ca0"
      },
      "cell_type": "markdown",
      "source": [
        "This follows what we did previously."
      ],
      "id": "105e5ebc78da7ca0"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J_5keRzS-eX",
        "outputId": "1723f02b-6c7e-4cbb-a839-a02fd0cb1e37"
      },
      "id": "9J_5keRzS-eX",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.78 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.3.78)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.4.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (2.11.10)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.104.2->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.104.2->langchain_openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.104.2->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.104.2->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.78->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.78->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.35-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-0.3.35\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-16T16:32:25.469284Z",
          "start_time": "2025-10-16T16:32:25.453647Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c76bf554da4f0c8",
        "outputId": "894c9455-2b40-4188-bf40-86a93757c697"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Imports\n",
        "# -----------------------------\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: Set your OpenAI API key\n",
        "# -----------------------------\n",
        "#  Replace 'sk-your_actual_key_here' with your actual OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"api key \"  # <-- Add your API key here\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: Initialize the LLM\n",
        "# -----------------------------\n",
        "llm_with_tools = ChatOpenAI(\n",
        "    model=\"gpt-4o\"\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 3: Define math tools\n",
        "# -----------------------------\n",
        "def add(inputs):\n",
        "    return inputs.get(\"a\") + inputs.get(\"b\")\n",
        "\n",
        "def subtract(inputs):\n",
        "    return inputs.get(\"a\") - inputs.get(\"b\")\n",
        "\n",
        "def multiply(inputs):\n",
        "    return inputs.get(\"a\") * inputs.get(\"b\")\n",
        "\n",
        "def divide(inputs):\n",
        "    b = inputs.get(\"b\")\n",
        "    if b == 0:\n",
        "        return \"Cannot divide by zero\"\n",
        "    return inputs.get(\"a\") / b\n",
        "\n",
        "# List of tools\n",
        "tools = [add, subtract, multiply, divide]\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4: System message\n",
        "# -----------------------------\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 5: Define assistant node\n",
        "# -----------------------------\n",
        "def assistant(state: MessagesState):\n",
        "    \"\"\"\n",
        "    This node takes the current state and calls the LLM with the system message\n",
        "    plus any user messages. Returns LLM response in 'messages'.\n",
        "    \"\"\"\n",
        "    messages_to_send = [sys_msg] + state[\"messages\"]\n",
        "    response = llm_with_tools.invoke(messages_to_send)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# -----------------------------\n",
        "# Step 6: Example usage\n",
        "# -----------------------------\n",
        "example_inputs = [\n",
        "    {\"a\": 7, \"b\": 3},\n",
        "    {\"a\": 12, \"b\": 4},\n",
        "    {\"a\": 5, \"b\": 0}  # Tests division by zero\n",
        "]\n",
        "\n",
        "for ex in example_inputs:\n",
        "    print(f\"Inputs: a={ex['a']}, b={ex['b']}\")\n",
        "    print(\"Addition:\", add(ex))\n",
        "    print(\"Subtraction:\", subtract(ex))\n",
        "    print(\"Multiplication:\", multiply(ex))\n",
        "    print(\"Division:\", divide(ex))\n",
        "    print(\"-\" * 30)\n"
      ],
      "id": "2c76bf554da4f0c8",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: a=7, b=3\n",
            "Addition: 10\n",
            "Subtraction: 4\n",
            "Multiplication: 21\n",
            "Division: 2.3333333333333335\n",
            "------------------------------\n",
            "Inputs: a=12, b=4\n",
            "Addition: 16\n",
            "Subtraction: 8\n",
            "Multiplication: 48\n",
            "Division: 3.0\n",
            "------------------------------\n",
            "Inputs: a=5, b=0\n",
            "Addition: 5\n",
            "Subtraction: 5\n",
            "Multiplication: 0\n",
            "Division: Cannot divide by zero\n",
            "------------------------------\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langgraph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp_Yxf1VTd7b",
        "outputId": "4ed92b80-d5b0-4bd0-8aca-1274f0f9882e"
      },
      "id": "hp_Yxf1VTd7b",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.10-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.78)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.6.10-py3-none-any.whl (155 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.6.10 langgraph-checkpoint-2.1.2 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.9 ormsgpack-1.11.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-16T16:32:25.477850Z",
          "start_time": "2025-10-16T16:32:25.473105Z"
        },
        "id": "f89afc4a2bad221b"
      },
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# System message\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
        "\n",
        "# Node\n",
        "def assistant(state: MessagesState):\n",
        "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
      ],
      "id": "f89afc4a2bad221b",
      "outputs": [],
      "execution_count": 9
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-16T16:32:25.881131Z",
          "start_time": "2025-10-16T16:32:25.481770Z"
        },
        "id": "8c9f49f13b388a4d"
      },
      "cell_type": "code",
      "source": [
        "def add(inputs):\n",
        "    \"\"\"Add two numbers a and b.\"\"\"\n",
        "    return inputs.get(\"a\") + inputs.get(\"b\")\n",
        "\n",
        "def subtract(inputs):\n",
        "    \"\"\"Subtract b from a.\"\"\"\n",
        "    return inputs.get(\"a\") - inputs.get(\"b\")\n",
        "\n",
        "def multiply(inputs):\n",
        "    \"\"\"Multiply a and b.\"\"\"\n",
        "    return inputs.get(\"a\") * inputs.get(\"b\")\n",
        "\n",
        "def divide(inputs):\n",
        "    \"\"\"Divide a by b. Returns error if b is zero.\"\"\"\n",
        "    b = inputs.get(\"b\")\n",
        "    if b == 0:\n",
        "        return \"Cannot divide by zero\"\n",
        "    return inputs.get(\"a\") / b\n"
      ],
      "id": "8c9f49f13b388a4d",
      "outputs": [],
      "execution_count": 11
    },
    {
      "metadata": {
        "id": "ac8234dde94aacf6"
      },
      "cell_type": "markdown",
      "source": [
        "## Memory\n",
        "\n",
        "Let's run our agent, as before."
      ],
      "id": "ac8234dde94aacf6"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-16T16:32:27.367482Z",
          "start_time": "2025-10-16T16:32:25.884411Z"
        },
        "id": "a1fe471ca2520af8",
        "outputId": "d3ff209d-625c-435b-9c98-5d195c740996"
      },
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
        "messages = react_graph.invoke({\"messages\": messages})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "id": "a1fe471ca2520af8",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 3 and 4.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  add (call_vu5bg3H5V0bTv9omQujOVNTV)\n",
            " Call ID: call_vu5bg3H5V0bTv9omQujOVNTV\n",
            "  Args:\n",
            "    a: 3\n",
            "    b: 4\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "7\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The sum of 3 and 4 is 7.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3f417c112e665af8"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's multiply by 2!"
      ],
      "id": "3f417c112e665af8"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-16T16:32:28.819586Z",
          "start_time": "2025-10-16T16:32:27.378909Z"
        },
        "id": "db1e57652343ec5e",
        "outputId": "99d37645-a0ce-4ea1-e538-e02970f3a1a3"
      },
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
        "messages = react_graph.invoke({\"messages\": messages})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "id": "db1e57652343ec5e",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply that by 2.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "It seems like you're referring to a previous result or number, but I don't have that context. Could you please specify the number you'd like to multiply by 2?\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "8b0373d2de7242f2"
      },
      "cell_type": "markdown",
      "source": [
        "We don't retain memory of 7 from our initial chat!\n",
        "\n",
        "This is because [state is transient](https://github.com/langchain-ai/langgraph/discussions/352#discussioncomment-9291220) to a single graph execution.\n",
        "\n",
        "Of course, this limits our ability to have multi-turn conversations with interruptions.\n",
        "\n",
        "We can use [persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) to address this!\n",
        "\n",
        "LangGraph can use a checkpointer to automatically save the graph state after each step.\n",
        "\n",
        "This built-in persistence layer gives us memory, allowing LangGraph to pick up from the last state update.\n",
        "\n",
        "One of the easiest checkpointers to use is the `MemorySaver`, an in-memory key-value store for Graph state.\n",
        "\n",
        "All we need to do is simply compile the graph with a checkpointer, and our graph has memory!"
      ],
      "id": "8b0373d2de7242f2"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-16T16:32:28.831766Z",
          "start_time": "2025-10-16T16:32:28.825878Z"
        },
        "id": "df2ab7d56a686ad1"
      },
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "memory = MemorySaver()\n",
        "react_graph_memory = builder.compile(checkpointer=memory)"
      ],
      "id": "df2ab7d56a686ad1",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "705428013885e8ab"
      },
      "cell_type": "markdown",
      "source": [
        "When we use memory, we need to specify a `thread_id`.\n",
        "\n",
        "This `thread_id` will store our collection of graph states.\n",
        "\n",
        "Here is a cartoon:\n",
        "\n",
        "* The checkpointer write the state at every step of the graph\n",
        "* These checkpoints are saved in a thread\n",
        "* We can access that thread in the future using the `thread_id`\n",
        "\n",
        "![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e0e9f526b41a4ed9e2d28b_agent-memory2.png)\n"
      ],
      "id": "705428013885e8ab"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-16T16:32:31.178324Z",
          "start_time": "2025-10-16T16:32:28.893620Z"
        },
        "id": "ae2984c9d7bbeda0",
        "outputId": "d39ae09d-8a6d-4752-8eb1-2eace2aba121"
      },
      "cell_type": "code",
      "source": [
        "# Specify a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Specify an input\n",
        "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
        "\n",
        "# Run\n",
        "messages = react_graph_memory.invoke({\"messages\": messages},config)\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "id": "ae2984c9d7bbeda0",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 3 and 4.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  add (call_TqXqiP0EBv06EpRLyRX7zdIW)\n",
            " Call ID: call_TqXqiP0EBv06EpRLyRX7zdIW\n",
            "  Args:\n",
            "    a: 3\n",
            "    b: 4\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "7\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The result of adding 3 and 4 is 7.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "98ba7d57adb25673"
      },
      "cell_type": "markdown",
      "source": [
        "If we pass the same `thread_id`, then we can proceed from from the previously logged state checkpoint!\n",
        "\n",
        "In this case, the above conversation is captured in the thread.\n",
        "\n",
        "The `HumanMessage` we pass (`\"Multiply that by 2.\"`) is appended to the above conversation.\n",
        "\n",
        "So, the model now know that `that` refers to the `The sum of 3 and 4 is 7.`."
      ],
      "id": "98ba7d57adb25673"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-16T16:32:32.859943Z",
          "start_time": "2025-10-16T16:32:31.184057Z"
        },
        "id": "e6d0aa1a3e89d670",
        "outputId": "47ee455d-1c21-4bd5-a2fd-0db156621803"
      },
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
        "messages = react_graph_memory.invoke({\"messages\": messages}, config)\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "id": "e6d0aa1a3e89d670",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 3 and 4.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  add (call_TqXqiP0EBv06EpRLyRX7zdIW)\n",
            " Call ID: call_TqXqiP0EBv06EpRLyRX7zdIW\n",
            "  Args:\n",
            "    a: 3\n",
            "    b: 4\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "7\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The result of adding 3 and 4 is 7.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply that by 2.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (call_PGvfhtdkdQenQC8pxghPeLpZ)\n",
            " Call ID: call_PGvfhtdkdQenQC8pxghPeLpZ\n",
            "  Args:\n",
            "    a: 7\n",
            "    b: 2\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "14\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The result of multiplying 7 by 2 is 14.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-16T16:32:34.715594Z",
          "start_time": "2025-10-16T16:32:32.877407Z"
        },
        "id": "8f876df4b95bc47",
        "outputId": "59ea36d6-c53f-4ba9-990a-7e2c5adba037"
      },
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"subtract that by 2.\")]\n",
        "messages = react_graph_memory.invoke({\"messages\": messages}, config)\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "id": "8f876df4b95bc47",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 3 and 4.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  add (call_TqXqiP0EBv06EpRLyRX7zdIW)\n",
            " Call ID: call_TqXqiP0EBv06EpRLyRX7zdIW\n",
            "  Args:\n",
            "    a: 3\n",
            "    b: 4\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "7\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The result of adding 3 and 4 is 7.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply that by 2.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (call_PGvfhtdkdQenQC8pxghPeLpZ)\n",
            " Call ID: call_PGvfhtdkdQenQC8pxghPeLpZ\n",
            "  Args:\n",
            "    a: 7\n",
            "    b: 2\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "14\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The result of multiplying 7 by 2 is 14.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "subtract that by 2.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  subtract (call_lLFVCW3oPB1MhnZ28ysovuQZ)\n",
            " Call ID: call_lLFVCW3oPB1MhnZ28ysovuQZ\n",
            "  Args:\n",
            "    a: 14\n",
            "    b: 2\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: subtract\n",
            "\n",
            "12\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The result of subtracting 2 from 14 is 12.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "9c53dfc4f3e5c673"
      },
      "cell_type": "markdown",
      "source": [
        "## LangGraph Studio\n",
        "\n",
        "\n",
        "**锔 DISCLAIMER**\n",
        "\n",
        "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `module-1/studio/` directory in this module:\n",
        "\n",
        "```\n",
        "langgraph dev\n",
        "```"
      ],
      "id": "9c53dfc4f3e5c673"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-16T16:32:34.723655Z",
          "start_time": "2025-10-16T16:32:34.721655Z"
        },
        "id": "f93a9042e6b60b65"
      },
      "cell_type": "code",
      "source": [],
      "id": "f93a9042e6b60b65",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}