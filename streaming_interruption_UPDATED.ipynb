{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0c9e547f",
      "metadata": {
        "id": "0c9e547f"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/streaming-interruption.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239464-lesson-1-streaming)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "319adfec-2d0a-49f2-87f9-275c4a32add2",
      "metadata": {
        "id": "319adfec-2d0a-49f2-87f9-275c4a32add2"
      },
      "source": [
        "# Streaming\n",
        "\n",
        "## Review\n",
        "\n",
        "In module 2, covered a few ways to customize graph state and memory.\n",
        "\n",
        "We built up to a Chatbot with external memory that can sustain long-running conversations.\n",
        "\n",
        "## Goals\n",
        "\n",
        "This module will dive into `human-in-the-loop`, which builds on memory and allows users to interact directly with graphs in various ways.\n",
        "\n",
        "To set the stage for `human-in-the-loop`, we'll first dive into streaming, which provides several ways to visualize graph output (e.g., node state or chat model tokens) over the course of execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa",
      "metadata": {
        "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_openai langgraph_sdk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d7e41b-c6ba-4e47-b645-6c110bede549",
      "metadata": {
        "id": "70d7e41b-c6ba-4e47-b645-6c110bede549"
      },
      "source": [
        "## Streaming\n",
        "\n",
        "LangGraph is built with [first class support for streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
        "\n",
        "Let's set up our Chatbot from Module 2, and show various way to stream outputs from the graph during execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5b430d92-f595-4322-a56e-06de7485daa8",
      "metadata": {
        "id": "5b430d92-f595-4322-a56e-06de7485daa8"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"Open ai API key to be used here\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0682fc",
      "metadata": {
        "id": "4d0682fc"
      },
      "source": [
        "Note that we use `RunnableConfig` with `call_model` to enable token-wise streaming. This is [only needed with python < 3.11](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/). We include in case you are running this notebook in CoLab, which will use python 3.x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
      "metadata": {
        "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
        "outputId": "d11df860-a66a-43ce-a39f-ea4e579357cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ==========================================\n",
        "#  OPENAI API KEY SETUP\n",
        "# ==========================================\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"open ai API key is to be used here\"  # 👈 Enter your API key\n",
        "\n",
        "# ==========================================\n",
        "#  IMPORTS\n",
        "# ==========================================\n",
        "from IPython.display import Image, display\n",
        "from typing import Literal\n",
        "import time, sys\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "# ==========================================\n",
        "#  LLM INITIALIZATION\n",
        "# ==========================================\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# ==========================================\n",
        "#  STATE DEFINITION\n",
        "# ==========================================\n",
        "class State(MessagesState):\n",
        "    summary: str\n",
        "    feedback: str\n",
        "    sentiment: str  # New field for visually different output\n",
        "\n",
        "# ==========================================\n",
        "# 💬 NODE: Conversation with Enhanced Streaming\n",
        "# ==========================================\n",
        "def call_model(state: State, config: RunnableConfig):\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "    feedback = state.get(\"feedback\", \"\")\n",
        "    sentiment = state.get(\"sentiment\", \"\")\n",
        "\n",
        "    system_content = \" Conversation Context:\\n\"\n",
        "    if summary:\n",
        "        system_content += f\"- Summary: {summary}\\n\"\n",
        "    if feedback:\n",
        "        system_content += f\"- User Feedback: {feedback}\\n\"\n",
        "    if sentiment:\n",
        "        system_content += f\"- Sentiment from last round: {sentiment}\\n\"\n",
        "\n",
        "    messages = [SystemMessage(content=system_content)] + state[\"messages\"]\n",
        "\n",
        "    print(\"\\n🔹 Streaming started from [conversation node] 🔹\\n\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    response = model.stream(messages, config=config)\n",
        "    full_text = \"\"\n",
        "    token_counter = 0\n",
        "\n",
        "    for chunk in response:\n",
        "        if chunk.content:\n",
        "            # Visually chunk tokens in groups for distinct display\n",
        "            print(chunk.content, end=\"\", flush=True)\n",
        "            full_text += chunk.content\n",
        "            token_counter += 1\n",
        "            if token_counter % 10 == 0:\n",
        "                print(\" •\", end=\"\", flush=True)\n",
        "                time.sleep(0.1)\n",
        "\n",
        "    print(\"\\n\\n✅ Streaming complete for [conversation node]\\n\")\n",
        "    return {\"messages\": [HumanMessage(content=full_text)]}\n",
        "\n",
        "# ==========================================\n",
        "# 🧾 NODE: Summarization with Context Tone\n",
        "# ==========================================\n",
        "def summarize_conversation(state: State):\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "    summary_message = (\n",
        "        f\"Summarize the following discussion clearly and briefly. \"\n",
        "        f\"Current summary: {summary if summary else 'None yet'}\"\n",
        "    )\n",
        "\n",
        "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
        "\n",
        "    print(\"\\n🟢 Generating summary stream from [summarize_conversation node]\\n\")\n",
        "    response = model.stream(messages)\n",
        "    summary_text = \"\"\n",
        "\n",
        "    for chunk in response:\n",
        "        if chunk.content:\n",
        "            print(chunk.content, end=\"\", flush=True)\n",
        "            summary_text += chunk.content\n",
        "\n",
        "    print(\"\\n\\n Summary generation completed.\\n\")\n",
        "\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
        "\n",
        "    return {\"summary\": summary_text, \"messages\": delete_messages, \"sentiment\": \"neutral\"}\n",
        "\n",
        "# ==========================================\n",
        "#  CONDITIONAL FLOW CONTROL\n",
        "# ==========================================\n",
        "def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
        "    messages = state[\"messages\"]\n",
        "    if len(messages) > 5:\n",
        "        return \"summarize_conversation\"\n",
        "    return END\n",
        "\n",
        "# ==========================================\n",
        "#  GRAPH WORKFLOW CREATION\n",
        "# ==========================================\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(\"conversation\", call_model)\n",
        "workflow.add_node(\"summarize_conversation\", summarize_conversation)\n",
        "workflow.add_edge(START, \"conversation\")\n",
        "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
        "workflow.add_edge(\"summarize_conversation\", END)\n",
        "\n",
        "# ==========================================\n",
        "#  MEMORY & GRAPH COMPILATION\n",
        "# ==========================================\n",
        "memory = MemorySaver()\n",
        "graph = workflow.compile(checkpointer=memory)\n",
        "\n",
        "# ==========================================\n",
        "# 📈 GRAPH VISUALIZATION\n",
        "# ==========================================\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-core langchain-openai langgraph langchain-community\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FJCVBokZehv3",
        "outputId": "1da626bf-91ea-48f9-be87-ace8e68c57f5"
      },
      "id": "FJCVBokZehv3",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.0.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.37)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.1)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.1)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.11.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain-1.0.2-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.4-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-text-splitters, langchain-classic, langchain-community, langchain\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.11\n",
            "    Uninstalling langchain-text-splitters-0.3.11:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-1.0.2 langchain-classic-1.0.0 langchain-community-0.4 langchain-text-splitters-1.0.0 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              },
              "id": "9b90b4ccd5ec45588bc19e3ad8abe211"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f847a787-b301-488c-9b58-cba9f389f55d",
      "metadata": {
        "id": "f847a787-b301-488c-9b58-cba9f389f55d"
      },
      "source": [
        "### Streaming full state\n",
        "\n",
        "Now, let's talk about ways to [stream our graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
        "\n",
        "`.stream` and `.astream` are sync and async methods for streaming back results.\n",
        "\n",
        "LangGraph supports a few [different streaming modes](https://langchain-ai.github.io/langgraph/how-tos/stream-values/) for [graph state](https://langchain-ai.github.io/langgraph/how-tos/stream-values/):\n",
        "\n",
        "* `values`: This streams the full state of the graph after each node is called.\n",
        "* `updates`: This streams updates to the state of the graph after each node is called.\n",
        "\n",
        "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
        "\n",
        "Let's look at `stream_mode=\"updates\"`.\n",
        "\n",
        "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
        "\n",
        "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community\n"
      ],
      "metadata": {
        "id": "WoTEkySZ048a"
      },
      "id": "WoTEkySZ048a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
      "metadata": {
        "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22"
      },
      "outputs": [],
      "source": [
        "# === Imports ===\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# === Step 1: Define conversation state ===\n",
        "class ConversationState(dict):\n",
        "    messages: list\n",
        "\n",
        "# === Step 2: Define mock model ===\n",
        "class MockLLM:\n",
        "    def __init__(self):\n",
        "        self.responses = [\n",
        "            \"Hello! I'm your AI assistant. How can I help you today?\",\n",
        "            \"Sure! Here's a summary of our discussion so far.\",\n",
        "            \"Glad I could help! Anything else you'd like to know?\"\n",
        "        ]\n",
        "        self.index = 0\n",
        "\n",
        "    def invoke(self, messages, config=None):\n",
        "        response = self.responses[self.index % len(self.responses)]\n",
        "        self.index += 1\n",
        "        return AIMessage(content=response)\n",
        "\n",
        "mock_model = MockLLM()\n",
        "\n",
        "# === Step 3: Define the conversation node ===\n",
        "def conversation_node(state: ConversationState):\n",
        "    user_message = state[\"messages\"][-1]\n",
        "    ai_response = mock_model.invoke([user_message])\n",
        "    state[\"messages\"].append(ai_response)\n",
        "    return state\n",
        "\n",
        "# === Step 4: Build the graph ===\n",
        "graph_builder = StateGraph(ConversationState)\n",
        "graph_builder.add_node(\"conversation\", conversation_node)\n",
        "\n",
        "# 🟢 Add entrypoint and exit\n",
        "graph_builder.add_edge(START, \"conversation\")\n",
        "graph_builder.add_edge(\"conversation\", END)\n",
        "\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "# === Step 5: Run conversation ===\n",
        "config = {\"configurable\": {\"thread_id\": \"offline_demo_01\"}}\n",
        "\n",
        "print(\"🔹 Streaming simulated conversation (no API key, offline mode) 🔹\\n\")\n",
        "\n",
        "# Simulate one turn of conversation\n",
        "for update in graph.stream({\"messages\": [HumanMessage(content=\"Hi there!\")]}, config, stream_mode=\"updates\"):\n",
        "    if \"conversation\" in update:\n",
        "        print(\"🧠 Conversation node updated →\", update[\"conversation\"][\"messages\"][-1].content)\n",
        "\n",
        "print(\"\\n✅ Offline simulation complete — mock AI conversation working without OpenAI key.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c4882e9-07dd-4d70-866b-dfc530418cad",
      "metadata": {
        "id": "0c4882e9-07dd-4d70-866b-dfc530418cad"
      },
      "source": [
        "Let's now just print the state update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c859c777-cb12-4682-9108-6b367e597b81",
      "metadata": {
        "id": "c859c777-cb12-4682-9108-6b367e597b81",
        "outputId": "30661c62-e3f1-4cca-89fc-2b171397a33e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Starting interactive streaming (offline simulation mode) 🔹\n",
            "\n",
            "\n",
            "🔹 Streaming started from [conversation node] 🔹\n",
            "\n",
            "Sure! 2 multiplied by 3 equals 6. \n",
            "✅ Streaming complete for [conversation node]\n",
            "\n",
            "\n",
            "🌀 Step 1: Node update detected...\n",
            "\n",
            "💬 Conversation node output:\n",
            "🤖 AI: Sure! 2 multiplied by 3 equals 6. \n",
            "\n",
            "✅ Streaming session completed — all updates simulated successfully.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 🧩 OFFLINE SIMULATED STREAMING CHAT WORKFLOW\n",
        "# ==========================================\n",
        "\n",
        "import time, sys, random\n",
        "from IPython.display import Image, display\n",
        "from typing import Literal\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "# ------------------------------------------\n",
        "# Simulated message classes (mocking LangChain)\n",
        "# ------------------------------------------\n",
        "@dataclass\n",
        "class HumanMessage:\n",
        "    content: str\n",
        "    id: str = field(default_factory=lambda: f\"user_{random.randint(1000,9999)}\")\n",
        "    role: str = \"user\"\n",
        "    def pretty_print(self):\n",
        "        print(f\"🧑 Human: {self.content}\")\n",
        "\n",
        "@dataclass\n",
        "class AIMessage:\n",
        "    content: str\n",
        "    id: str = field(default_factory=lambda: f\"ai_{random.randint(1000,9999)}\")\n",
        "    role: str = \"assistant\"\n",
        "    def pretty_print(self):\n",
        "        print(f\"🤖 AI: {self.content}\")\n",
        "\n",
        "@dataclass\n",
        "class RemoveMessage:\n",
        "    id: str\n",
        "\n",
        "@dataclass\n",
        "class SystemMessage:\n",
        "    content: str\n",
        "    role: str = \"system\"\n",
        "\n",
        "# ------------------------------------------\n",
        "# Simulated LLM model (no API)\n",
        "# ------------------------------------------\n",
        "class MockStreamResponse:\n",
        "    \"\"\"Simulates an OpenAI streaming response.\"\"\"\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "    def __iter__(self):\n",
        "        for word in self.text.split():\n",
        "            yield type(\"Chunk\", (), {\"content\": word + \" \"})\n",
        "            time.sleep(0.05)\n",
        "\n",
        "class MockChatModel:\n",
        "    \"\"\"Offline mock of ChatOpenAI model.\"\"\"\n",
        "    def __init__(self, name=\"mock-gpt\", temperature=0):\n",
        "        self.name = name\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def stream(self, messages, config=None):\n",
        "        # Extract last human input\n",
        "        prompt = messages[-1].content if messages else \"Hello!\"\n",
        "        simulated_response = self._generate_response(prompt)\n",
        "        return MockStreamResponse(simulated_response)\n",
        "\n",
        "    def _generate_response(self, prompt):\n",
        "        # Simulate some reasoning and reply\n",
        "        if \"multiply\" in prompt.lower():\n",
        "            return \"Sure! 2 multiplied by 3 equals 6.\"\n",
        "        elif \"summarize\" in prompt.lower():\n",
        "            return \"Here’s a short summary of your chat: You asked for help summarizing messages.\"\n",
        "        else:\n",
        "            return \"I’m here to simulate an LLM response offline!\"\n",
        "\n",
        "# ------------------------------------------\n",
        "# Define State class\n",
        "# ------------------------------------------\n",
        "@dataclass\n",
        "class State:\n",
        "    messages: list\n",
        "    summary: str = \"\"\n",
        "    feedback: str = \"\"\n",
        "    sentiment: str = \"\"\n",
        "\n",
        "# ------------------------------------------\n",
        "# Node 1: Conversation simulation\n",
        "# ------------------------------------------\n",
        "def call_model(state: State, config=None):\n",
        "    summary = state.summary\n",
        "    feedback = state.feedback\n",
        "    sentiment = state.sentiment\n",
        "\n",
        "    print(\"\\n🔹 Streaming started from [conversation node] 🔹\\n\")\n",
        "\n",
        "    system_content = f\"Summary: {summary}\\nFeedback: {feedback}\\nSentiment: {sentiment}\\n\"\n",
        "    messages = [SystemMessage(content=system_content)] + state.messages\n",
        "\n",
        "    response = model.stream(messages)\n",
        "    full_text = \"\"\n",
        "    token_counter = 0\n",
        "\n",
        "    for chunk in response:\n",
        "        print(chunk.content, end=\"\", flush=True)\n",
        "        full_text += chunk.content\n",
        "        token_counter += 1\n",
        "        if token_counter % 10 == 0:\n",
        "            print(\" •\", end=\"\", flush=True)\n",
        "    print(\"\\n✅ Streaming complete for [conversation node]\\n\")\n",
        "\n",
        "    return {\"messages\": [AIMessage(content=full_text)]}\n",
        "\n",
        "# ------------------------------------------\n",
        "# Node 2: Summarization simulation\n",
        "# ------------------------------------------\n",
        "def summarize_conversation(state: State):\n",
        "    print(\"\\n🟢 Generating summary stream from [summarize_conversation node]\\n\")\n",
        "    response = model.stream([HumanMessage(content=\"Summarize conversation\")])\n",
        "    summary_text = \"\"\n",
        "    for chunk in response:\n",
        "        print(chunk.content, end=\"\", flush=True)\n",
        "        summary_text += chunk.content\n",
        "    print(\"\\n✅ Summary generation completed.\\n\")\n",
        "    return {\"summary\": summary_text, \"sentiment\": \"neutral\"}\n",
        "\n",
        "# ------------------------------------------\n",
        "# Conditional flow logic\n",
        "# ------------------------------------------\n",
        "def should_continue(state: State) -> Literal[\"summarize_conversation\", \"end\"]:\n",
        "    if len(state.messages) > 2:\n",
        "        return \"summarize_conversation\"\n",
        "    return \"end\"\n",
        "\n",
        "# ------------------------------------------\n",
        "# Graph simulation (offline)\n",
        "# ------------------------------------------\n",
        "class MockGraph:\n",
        "    def __init__(self):\n",
        "        self.nodes = {\"conversation\": call_model, \"summarize_conversation\": summarize_conversation}\n",
        "    def stream(self, inputs, config=None, stream_mode=\"updates\"):\n",
        "        state = State(messages=inputs[\"messages\"])\n",
        "        yield {\"conversation\": self.nodes[\"conversation\"](state)}\n",
        "        if should_continue(state) == \"summarize_conversation\":\n",
        "            yield {\"summarize_conversation\": self.nodes[\"summarize_conversation\"](state)}\n",
        "\n",
        "# ------------------------------------------\n",
        "# Instantiate mock model & graph\n",
        "# ------------------------------------------\n",
        "model = MockChatModel()\n",
        "graph = MockGraph()\n",
        "\n",
        "# ------------------------------------------\n",
        "# Run streaming simulation\n",
        "# ------------------------------------------\n",
        "print(\"🔹 Starting interactive streaming (offline simulation mode) 🔹\\n\")\n",
        "user_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "\n",
        "for step_count, chunk in enumerate(graph.stream({\"messages\": [user_message]})):\n",
        "    print(f\"\\n🌀 Step {step_count + 1}: Node update detected...\\n\")\n",
        "    if \"conversation\" in chunk:\n",
        "        print(\"💬 Conversation node output:\")\n",
        "        for msg in chunk[\"conversation\"][\"messages\"]:\n",
        "            msg.pretty_print()\n",
        "    if \"summarize_conversation\" in chunk:\n",
        "        print(\"\\n📄 Summary node output:\")\n",
        "        print(chunk[\"summarize_conversation\"][\"summary\"])\n",
        "\n",
        "print(\"\\n✅ Streaming session completed — all updates simulated successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "583bf219-6358-4d06-ae99-c40f43569fda",
      "metadata": {
        "id": "583bf219-6358-4d06-ae99-c40f43569fda"
      },
      "source": [
        "Now, we can see `stream_mode=\"values\"`.\n",
        "\n",
        "This is the `full state` of the graph after the `conversation` node is called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
      "metadata": {
        "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
        "outputId": "207ff9f4-da40-486a-cedd-5d155022cf20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Streaming full state values from each node (Offline Simulation) 🔹\n",
            "\n",
            "\n",
            "🔹 Streaming started from [conversation node] 🔹\n",
            "\n",
            "I’m here to simulate an LLM response offline! \n",
            "✅ Streaming complete for [conversation node]\n",
            "\n",
            "🧩 Step 1: Full graph state received\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "✅ Streaming with 'values' mode complete — full state updates captured.\n"
          ]
        }
      ],
      "source": [
        "# === Start a new simulated conversation stream (Offline + Visualized Mode) ===\n",
        "print(\"🔹 Streaming full state values from each node (Offline Simulation) 🔹\\n\")\n",
        "\n",
        "# Unique thread for this session\n",
        "config = {\"configurable\": {\"thread_id\": \"session_2\"}}\n",
        "\n",
        "# Define user input\n",
        "input_message = HumanMessage(content=\"Hey, this is Alex! Let's start a short chat.\")\n",
        "\n",
        "# Stream with 'values' mode to see full state after each node execution\n",
        "for step, event in enumerate(graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\")):\n",
        "    print(f\"🧩 Step {step + 1}: Full graph state received\")\n",
        "\n",
        "    # Show messages in this step\n",
        "    messages = event.get(\"messages\", [])\n",
        "    if messages:\n",
        "        print(\"💬 Conversation Messages:\")\n",
        "        for msg in messages:\n",
        "            try:\n",
        "                msg.pretty_print()\n",
        "            except AttributeError:\n",
        "                print(f\"🧾 Message Content: {msg}\")\n",
        "\n",
        "    # If summary key exists, display it\n",
        "    if \"summary\" in event:\n",
        "        print(f\"📄 Current Summary Snapshot:\\n{event['summary']}\\n\")\n",
        "\n",
        "    print(\"─\" * 80)\n",
        "\n",
        "print(\"\\n✅ Streaming with 'values' mode complete — full state updates captured.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7",
      "metadata": {
        "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7"
      },
      "source": [
        "### Streaming tokens\n",
        "\n",
        "We often want to stream more than graph state.\n",
        "\n",
        "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
        "\n",
        "We can do this [using the `.astream_events` method](https://langchain-ai.github.io/langgraph/how-tos/streaming-from-final-node/#stream-outputs-from-the-final-node), which streams back events as they happen inside nodes!\n",
        "\n",
        "Each event is a dict with a few keys:\n",
        "\n",
        "* `event`: This is the type of event that is being emitted.\n",
        "* `name`: This is the name of event.\n",
        "* `data`: This is the data associated with the event.\n",
        "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
        "\n",
        "Let's have a look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
      "metadata": {
        "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
        "outputId": "01584557-acfc-4e62-b2a6-781f3a71e1b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Starting async event stream — visualizing graph events in real-time 🔹\n",
            "\n",
            " Step 1 | Node: conversation\n",
            " Event Type: on_start | Name: stream_begin\n",
            " Data (preview): {'thread_id': 'session_3'}\n",
            "----------------------------------------------------------------------\n",
            " Step 2 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): Barcelona\n",
            "----------------------------------------------------------------------\n",
            " Step 3 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): FC\n",
            "----------------------------------------------------------------------\n",
            " Step 4 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): was\n",
            "----------------------------------------------------------------------\n",
            " Step 5 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): founded\n",
            "----------------------------------------------------------------------\n",
            " Step 6 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): in\n",
            "----------------------------------------------------------------------\n",
            " Step 7 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): 1899.\n",
            "----------------------------------------------------------------------\n",
            " Step 8 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): They\n",
            "----------------------------------------------------------------------\n",
            " Step 9 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): have\n",
            "----------------------------------------------------------------------\n",
            " Step 10 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): the\n",
            "----------------------------------------------------------------------\n",
            " Step 11 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): motto\n",
            "----------------------------------------------------------------------\n",
            " Step 12 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): 'Més\n",
            "----------------------------------------------------------------------\n",
            " Step 13 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): que\n",
            "----------------------------------------------------------------------\n",
            " Step 14 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): un\n",
            "----------------------------------------------------------------------\n",
            " Step 15 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): club'.\n",
            "----------------------------------------------------------------------\n",
            " Step 16 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): Camp\n",
            "----------------------------------------------------------------------\n",
            " Step 17 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): Nou\n",
            "----------------------------------------------------------------------\n",
            " Step 18 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): is\n",
            "----------------------------------------------------------------------\n",
            " Step 19 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): one\n",
            "----------------------------------------------------------------------\n",
            " Step 20 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): of\n",
            "----------------------------------------------------------------------\n",
            " Step 21 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): the\n",
            "----------------------------------------------------------------------\n",
            " Step 22 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): largest\n",
            "----------------------------------------------------------------------\n",
            " Step 23 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): stadiums\n",
            "----------------------------------------------------------------------\n",
            " Step 24 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): in\n",
            "----------------------------------------------------------------------\n",
            " Step 25 | Node: conversation\n",
            " Event Type: on_chat_model_stream | Name: token_chunk\n",
            " Data (preview): Europe.\n",
            "----------------------------------------------------------------------\n",
            " Step 26 | Node: conversation\n",
            " Event Type: on_end | Name: stream_end\n",
            " Data (preview): {'result': 'Streaming completed successfully.'}\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "✅ Async streaming complete — all node events processed.\n"
          ]
        }
      ],
      "source": [
        "import asyncio, time, random\n",
        "\n",
        "# ==========================================\n",
        "#  MOCK CLASSES (reuse from before)\n",
        "# ==========================================\n",
        "class HumanMessage:\n",
        "    def __init__(self, content):\n",
        "        self.content = content\n",
        "        self.role = \"user\"\n",
        "    def pretty_print(self):\n",
        "        print(f\"🧑 Human: {self.content}\")\n",
        "\n",
        "class AIMessage:\n",
        "    def __init__(self, content):\n",
        "        self.content = content\n",
        "        self.role = \"assistant\"\n",
        "    def pretty_print(self):\n",
        "        print(f\"🤖 AI: {self.content}\")\n",
        "\n",
        "class MockChatModel:\n",
        "    def stream(self, messages, config=None):\n",
        "        prompt = messages[-1].content.lower()\n",
        "        if \"barcelona\" in prompt:\n",
        "            response = (\n",
        "                \"Barcelona FC was founded in 1899. They have the motto 'Més que un club'. \"\n",
        "                \"Camp Nou is one of the largest stadiums in Europe.\"\n",
        "            )\n",
        "        else:\n",
        "            response = \"Simulated streaming response...\"\n",
        "        for word in response.split():\n",
        "            yield {\"content\": word}\n",
        "            time.sleep(0.05)\n",
        "\n",
        "# ==========================================\n",
        "#  MOCK GRAPH with ASYNC EVENT STREAM\n",
        "# ==========================================\n",
        "class MockGraph:\n",
        "    def __init__(self):\n",
        "        self.model = MockChatModel()\n",
        "\n",
        "    async def astream_events(self, inputs, config=None, version=\"v2\"):\n",
        "        \"\"\"Simulated async event stream generator.\"\"\"\n",
        "        messages = inputs.get(\"messages\", [])\n",
        "        prompt = messages[-1].content if messages else \"\"\n",
        "\n",
        "        # Step 1: Emit start event\n",
        "        yield {\n",
        "            \"event\": \"on_start\",\n",
        "            \"metadata\": {\"langgraph_node\": \"conversation\"},\n",
        "            \"name\": \"stream_begin\",\n",
        "            \"data\": {\"thread_id\": config.get(\"configurable\", {}).get(\"thread_id\", \"unknown\")},\n",
        "        }\n",
        "        await asyncio.sleep(0.3)\n",
        "\n",
        "        # Step 2: Stream token events (simulating real-time output)\n",
        "        for i, word in enumerate(self.model.stream(messages)):\n",
        "            yield {\n",
        "                \"event\": \"on_chat_model_stream\",\n",
        "                \"metadata\": {\"langgraph_node\": \"conversation\"},\n",
        "                \"name\": \"token_chunk\",\n",
        "                \"data\": word[\"content\"],\n",
        "            }\n",
        "            await asyncio.sleep(0.05)\n",
        "\n",
        "        # Step 3: Emit final message complete event\n",
        "        yield {\n",
        "            \"event\": \"on_end\",\n",
        "            \"metadata\": {\"langgraph_node\": \"conversation\"},\n",
        "            \"name\": \"stream_end\",\n",
        "            \"data\": {\"result\": \"Streaming completed successfully.\"},\n",
        "        }\n",
        "\n",
        "# ==========================================\n",
        "#  MOCK GRAPH INSTANCE\n",
        "# ==========================================\n",
        "graph = MockGraph()\n",
        "\n",
        "# ==========================================\n",
        "#  ASYNC EVENT VISUALIZATION\n",
        "# ==========================================\n",
        "print(\"🔹 Starting async event stream — visualizing graph events in real-time 🔹\\n\")\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"session_3\"}}\n",
        "input_message = HumanMessage(content=\"Can you give me the coolest unique facts about the Barcelona football team?\")\n",
        "\n",
        "async def display_events():\n",
        "    step = 0\n",
        "    async for event in graph.astream_events({\"messages\": [input_message]}, config=config, version=\"v2\"):\n",
        "        step += 1\n",
        "        node_name = event[\"metadata\"].get(\"langgraph_node\", \"unknown\")\n",
        "        event_type = event.get(\"event\", \"unknown\")\n",
        "        event_name = event.get(\"name\", \"unnamed\")\n",
        "        data_preview = str(event.get(\"data\", \"\"))[:80]\n",
        "\n",
        "        print(f\" Step {step} | Node: {node_name}\")\n",
        "        print(f\" Event Type: {event_type} | Name: {event_name}\")\n",
        "        print(f\" Data (preview): {data_preview}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    print(\"\\n✅ Async streaming complete — all node events processed.\")\n",
        "\n",
        "# Run safely in Jupyter or Colab\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "await display_events()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d",
      "metadata": {
        "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d"
      },
      "source": [
        "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
        "\n",
        "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
        "\n",
        "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
      "metadata": {
        "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
        "outputId": "39880f13-b16b-42a4-bdc4-5d96e09b5259",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Simulating token-level streaming from the conversation node 🔹\n",
            "\n",
            "🧩 Token 1: Simulated\n",
            "🧩 Token 2: streaming\n",
            "🧩 Token 3: response...\n",
            "\n",
            "✅ Token streaming simulation complete — all tokens received from node: conversation\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "# === Stream model tokens selectively from a specific node (Offline Mode) ===\n",
        "print(\"🔹 Simulating token-level streaming from the conversation node 🔹\\n\")\n",
        "\n",
        "node_to_stream = \"conversation\"\n",
        "config = {\"configurable\": {\"thread_id\": \"session_4\"}}\n",
        "input_message = HumanMessage(content=\"Give me a short intro about the San Francisco 49ers football team.\")\n",
        "\n",
        "async def stream_node_tokens():\n",
        "    token_count = 0\n",
        "    async for event in graph.astream_events(\n",
        "        {\"messages\": [input_message]}, config=config, version=\"v2\"\n",
        "    ):\n",
        "        # Focus only on chat model streaming events\n",
        "        if (\n",
        "            event.get(\"event\") == \"on_chat_model_stream\"\n",
        "            and event[\"metadata\"].get(\"langgraph_node\") == node_to_stream\n",
        "        ):\n",
        "            token = event[\"data\"]\n",
        "            token_count += 1\n",
        "            print(f\"🧩 Token {token_count}: {token}\")\n",
        "\n",
        "    print(\"\\n✅ Token streaming simulation complete — all tokens received from node:\", node_to_stream)\n",
        "\n",
        "# Run safely inside Jupyter\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "await stream_node_tokens()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "226e569a-76c3-43d8-8f89-3ae687efde1c",
      "metadata": {
        "id": "226e569a-76c3-43d8-8f89-3ae687efde1c"
      },
      "source": [
        "As you see above, just use the `chunk` key to get the `AIMessageChunk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
      "metadata": {
        "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
        "outputId": "2bc438f1-aef4-4c4c-f2f8-994955cf8606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulated | streaming | response... | \n",
            "\n",
            "✅ Offline simulated token stream complete.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "#  ASYNC TOKEN STREAM (OFFLINE SIMULATION)\n",
        "# ==========================================\n",
        "node_to_stream = \"conversation\"\n",
        "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
        "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
        "\n",
        "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
        "    # Stream only from a specific node (conversation)\n",
        "    if event[\"event\"] == \"on_chat_model_stream\" and event[\"metadata\"].get(\"langgraph_node\", \"\") == node_to_stream:\n",
        "        data = event[\"data\"]  # already a text token (string)\n",
        "        print(data, end=\" | \")\n",
        "\n",
        "print(\"\\n\\n✅ Offline simulated token stream complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db",
      "metadata": {
        "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db"
      },
      "source": [
        "### Streaming with LangGraph API\n",
        "\n",
        "**⚠️ Notice**\n",
        "\n",
        "Since filming these videos, we've updated Studio so that it can now be run locally and accessed through your browser. This is the preferred way to run Studio instead of using the Desktop App shown in the video. It is now called _LangSmith Studio_ instead of _LangGraph Studio_. Detailed setup instructions are available in the \"Getting Setup\" guide at the start of the course. You can find a description of Studio [here](https://docs.langchain.com/langsmith/studio), and specific details for local deployment [here](https://docs.langchain.com/langsmith/quick-start-studio#local-development-server).  \n",
        "To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
        "\n",
        "```\n",
        "langgraph dev\n",
        "```\n",
        "\n",
        "You should see the following output:\n",
        "```\n",
        "- 🚀 API: http://127.0.0.1:2024\n",
        "- 🎨 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
        "- 📚 API Docs: http://127.0.0.1:2024/docs\n",
        "```\n",
        "\n",
        "Open your browser and navigate to the **Studio UI** URL shown above.\n",
        "\n",
        "The LangGraph API [supports editing graph state](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_edit_state/#initial-invocation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8925b632-512b-48e1-9220-61c06bfbf0b8",
      "metadata": {
        "id": "8925b632-512b-48e1-9220-61c06bfbf0b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870a767d-32d3-4e54-a853-3e3378a3d082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ LangGraph Studio features are limited on Google Colab. Some functionalities may not work properly.\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    print(\"⚠️ LangGraph Studio features are limited on Google Colab. Some functionalities may not work properly.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "079c2ad6",
      "metadata": {
        "id": "079c2ad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b070c710-3b94-4103-b38f-2a56ea5139bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Offline assistant list (simulated):\n",
            "• Offline Assistant — ID: mock_001 — Status: local\n",
            "• Test Workflow — ID: mock_002 — Status: simulated\n"
          ]
        }
      ],
      "source": [
        "# === Mocked offline version of LangGraph client ===\n",
        "\n",
        "class MockLangGraphClient:\n",
        "    class assistants:\n",
        "        @staticmethod\n",
        "        async def search():\n",
        "            return [\n",
        "                {\"id\": \"mock_001\", \"name\": \"Offline Assistant\", \"status\": \"local\"},\n",
        "                {\"id\": \"mock_002\", \"name\": \"Test Workflow\", \"status\": \"simulated\"}\n",
        "            ]\n",
        "\n",
        "# Instantiate mock client instead of real one\n",
        "client = MockLangGraphClient()\n",
        "\n",
        "# Simulate async search\n",
        "import asyncio\n",
        "assistants = asyncio.run(client.assistants.search())\n",
        "\n",
        "print(\"✅ Offline assistant list (simulated):\")\n",
        "for a in assistants:\n",
        "    print(f\"• {a['name']} — ID: {a['id']} — Status: {a['status']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32",
      "metadata": {
        "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32"
      },
      "source": [
        "Let's [stream `values`](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/), like before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
      "metadata": {
        "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
        "outputId": "cb381582-2e61-4d4a-e35c-d9e1a43a5f50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 Creating new mock thread...\n",
            "\n",
            "🎯 Running assistant 'agent' on thread 'mock-thread-001'\n",
            "💬 Input message: Multiply 2 and 3\n",
            "\n",
            "[THINKING] → Processing your input...\n",
            "[INTERMEDIATE] → Analyzing context and intent...\n",
            "[FINAL] → ✅ The result of 2 × 3 is 6.\n",
            "\n",
            "🎉 Offline simulation complete — behavior verified without any API key or server.\n"
          ]
        }
      ],
      "source": [
        "# === 🧠 Offline LangGraph SDK Simulation (Alternate Version) ===\n",
        "# This version simulates LangGraph behavior without needing API keys or a live server.\n",
        "# It shows how threads, streaming, and assistant logic work conceptually.\n",
        "\n",
        "import asyncio\n",
        "\n",
        "# === Step 1: Create a mock LangGraph client ===\n",
        "class MockLangGraphClient:\n",
        "    class threads:\n",
        "        @staticmethod\n",
        "        async def create():\n",
        "            print(\"📘 Creating new mock thread...\")\n",
        "            await asyncio.sleep(0.3)\n",
        "            return {\"thread_id\": \"mock-thread-001\"}\n",
        "\n",
        "    class runs:\n",
        "        @staticmethod\n",
        "        async def stream(thread_id, assistant_id, input, stream_mode):\n",
        "            print(f\"\\n🎯 Running assistant '{assistant_id}' on thread '{thread_id}'\")\n",
        "            print(f\"💬 Input message: {input['messages'][0]['content']}\\n\")\n",
        "\n",
        "            # Simulate the assistant thinking and responding gradually\n",
        "            responses = [\n",
        "                {\"event\": \"thinking\", \"data\": \"Processing your input...\"},\n",
        "                {\"event\": \"intermediate\", \"data\": \"Analyzing context and intent...\"},\n",
        "                {\"event\": \"final\", \"data\": \"✅ The result of 2 × 3 is 6.\"}\n",
        "            ]\n",
        "\n",
        "            for event in responses:\n",
        "                await asyncio.sleep(0.7)\n",
        "                yield event\n",
        "\n",
        "# === Step 2: Create the mock client instance ===\n",
        "client = MockLangGraphClient()\n",
        "\n",
        "# === Step 3: Define the async workflow ===\n",
        "async def run_simulated():\n",
        "    # Step 3.1: Create a new thread (like starting a new chat session)\n",
        "    thread = await client.threads.create()\n",
        "\n",
        "    # Step 3.2: Define input message\n",
        "    input_message = {\"content\": \"Multiply 2 and 3\"}\n",
        "\n",
        "    # Step 3.3: Stream simulated assistant output\n",
        "    async for event in client.runs.stream(\n",
        "        thread[\"thread_id\"],\n",
        "        assistant_id=\"agent\",\n",
        "        input={\"messages\": [input_message]},\n",
        "        stream_mode=\"values\"\n",
        "    ):\n",
        "        print(f\"[{event['event'].upper()}] → {event['data']}\")\n",
        "\n",
        "    print(\"\\n🎉 Offline simulation complete — behavior verified without any API key or server.\")\n",
        "\n",
        "# === Step 4: Run the simulation (Jupyter/Colab compatible) ===\n",
        "await run_simulated()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "556dc7fd-1cae-404f-816a-f13d772b3b14",
      "metadata": {
        "id": "556dc7fd-1cae-404f-816a-f13d772b3b14"
      },
      "source": [
        "The streamed objects have:\n",
        "\n",
        "* `event`: Type\n",
        "* `data`: State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "57b735aa-139c-45a3-a850-63519c0004f0",
      "metadata": {
        "id": "57b735aa-139c-45a3-a850-63519c0004f0",
        "outputId": "cb08c7d9-ad65-4ef2-f8f6-4cc5cab4e3c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧩 Creating a new mock thread...\n",
            "\n",
            "🚀 Simulating run on thread 'mock-thread-002' for assistant 'agent'...\n",
            "User said: Multiply 2 and 3\n",
            "Thinking...\n",
            "==============================\n",
            "Calculating result...\n",
            "==============================\n",
            "✅ The product of 2 and 3 is 6.\n",
            "==============================\n",
            "\n",
            "🎉 Offline mock stream complete — logic validated without any API dependency.\n"
          ]
        }
      ],
      "source": [
        "# === 🧠 Offline Async Simulation for LangGraph Streaming ===\n",
        "# This alternate version runs locally without API keys or SDK dependencies.\n",
        "# It simulates the behavior of `client.runs.stream()` and message conversion.\n",
        "\n",
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "from langchain_core.messages import HumanMessage, AIMessage, convert_to_messages\n",
        "\n",
        "# === Step 1: Define mock LangGraph-like client ===\n",
        "@dataclass\n",
        "class MockThread:\n",
        "    thread_id: str\n",
        "\n",
        "class MockLangGraphClient:\n",
        "    class threads:\n",
        "        @staticmethod\n",
        "        async def create():\n",
        "            print(\"🧩 Creating a new mock thread...\")\n",
        "            await asyncio.sleep(0.2)\n",
        "            return MockThread(thread_id=\"mock-thread-002\")\n",
        "\n",
        "    class runs:\n",
        "        @staticmethod\n",
        "        async def stream(thread_id, assistant_id, input, stream_mode):\n",
        "            print(f\"\\n🚀 Simulating run on thread '{thread_id}' for assistant '{assistant_id}'...\")\n",
        "            print(f\"User said: {input['messages'][0].content}\")\n",
        "            await asyncio.sleep(0.5)\n",
        "\n",
        "            # Simulate streaming output like a real LangGraph event\n",
        "            responses = [\n",
        "                {\"data\": {\"messages\": [AIMessage(content=\"Thinking...\")]}},\n",
        "                {\"data\": {\"messages\": [AIMessage(content=\"Calculating result...\")]}},\n",
        "                {\"data\": {\"messages\": [AIMessage(content=\"✅ The product of 2 and 3 is 6.\")]}},\n",
        "            ]\n",
        "\n",
        "            for event in responses:\n",
        "                await asyncio.sleep(0.6)\n",
        "                yield event\n",
        "\n",
        "# === Step 2: Create mock client instance ===\n",
        "client = MockLangGraphClient()\n",
        "\n",
        "# === Step 3: Async main execution ===\n",
        "async def run_simulated():\n",
        "    # Step 3.1: Create thread (mock)\n",
        "    thread = await client.threads.create()\n",
        "\n",
        "    # Step 3.2: Create human message input\n",
        "    input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "\n",
        "    # Step 3.3: Simulate streaming conversation\n",
        "    async for event in client.runs.stream(\n",
        "        thread.thread_id,\n",
        "        assistant_id=\"agent\",\n",
        "        input={\"messages\": [input_message]},\n",
        "        stream_mode=\"values\"\n",
        "    ):\n",
        "        messages = event[\"data\"].get(\"messages\", None)\n",
        "        if messages:\n",
        "            # Convert and print the last message for readability\n",
        "            print(convert_to_messages(messages)[-1].content)\n",
        "        print(\"=\" * 30)\n",
        "\n",
        "    print(\"\\n🎉 Offline mock stream complete — logic validated without any API dependency.\")\n",
        "\n",
        "# === Step 4: Run in Jupyter/Colab safely ===\n",
        "await run_simulated()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a555d186-27be-4ddf-934c-895a3105035d",
      "metadata": {
        "id": "a555d186-27be-4ddf-934c-895a3105035d"
      },
      "source": [
        "There are some new streaming mode that are only supported via the API.\n",
        "\n",
        "For example, we can [use `messages` mode](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/) to better handle the above case!\n",
        "\n",
        "This mode currently assumes that you have a `messages` key in your graph, which is a list of messages.\n",
        "\n",
        "All events emitted using `messages` mode have two attributes:\n",
        "\n",
        "* `event`: This is the name of the event\n",
        "* `data`: This is data associated with the event"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
      "metadata": {
        "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
        "outputId": "dcd2e424-bdcf-4e7e-ded2-818f178f93c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧩 Creating new simulated thread...\n",
            "\n",
            "🚀 Simulating stream on thread 'mock-thread-003' for assistant 'agent'...\n",
            "📩 User Input: Multiply 2 and 3\n",
            "🟢 Event emitted: on_node_start\n",
            "🟢 Event emitted: on_chat_model_start\n",
            "🟢 Event emitted: on_chat_model_stream\n",
            "🟢 Event emitted: on_chat_model_end\n",
            "🟢 Event emitted: on_node_end\n",
            "🟢 Event emitted: on_chain_complete\n",
            "\n",
            "✅ Simulation complete — event stream successfully mimicked.\n"
          ]
        }
      ],
      "source": [
        "# === 🧠 Offline Async Simulation: Stream Event Types (Alternate Version) ===\n",
        "# This version simulates client.runs.stream() and prints event types like LangGraph.\n",
        "\n",
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# === Step 1: Define mock LangGraph-like client ===\n",
        "@dataclass\n",
        "class MockThread:\n",
        "    thread_id: str\n",
        "\n",
        "class MockLangGraphClient:\n",
        "    class threads:\n",
        "        @staticmethod\n",
        "        async def create():\n",
        "            print(\"🧩 Creating new simulated thread...\")\n",
        "            await asyncio.sleep(0.2)\n",
        "            return MockThread(thread_id=\"mock-thread-003\")\n",
        "\n",
        "    class runs:\n",
        "        @staticmethod\n",
        "        async def stream(thread_id, assistant_id, input, stream_mode):\n",
        "            print(f\"\\n🚀 Simulating stream on thread '{thread_id}' for assistant '{assistant_id}'...\")\n",
        "            print(f\"📩 User Input: {input['messages'][0].content}\")\n",
        "            await asyncio.sleep(0.5)\n",
        "\n",
        "            # Simulated event sequence like LangGraph's real-time stream\n",
        "            mock_events = [\n",
        "                {\"event\": \"on_node_start\"},\n",
        "                {\"event\": \"on_chat_model_start\"},\n",
        "                {\"event\": \"on_chat_model_stream\"},\n",
        "                {\"event\": \"on_chat_model_end\"},\n",
        "                {\"event\": \"on_node_end\"},\n",
        "                {\"event\": \"on_chain_complete\"},\n",
        "            ]\n",
        "\n",
        "            for event in mock_events:\n",
        "                await asyncio.sleep(0.4)\n",
        "                yield event\n",
        "\n",
        "# === Step 2: Create client instance ===\n",
        "client = MockLangGraphClient()\n",
        "\n",
        "# === Step 3: Run the streaming simulation ===\n",
        "async def run_event_stream():\n",
        "    # Create a new thread\n",
        "    thread = await client.threads.create()\n",
        "\n",
        "    # Create human input\n",
        "    input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "\n",
        "    # Stream simulated events\n",
        "    async for event in client.runs.stream(\n",
        "        thread.thread_id,\n",
        "        assistant_id=\"agent\",\n",
        "        input={\"messages\": [input_message]},\n",
        "        stream_mode=\"messages\"\n",
        "    ):\n",
        "        print(f\"🟢 Event emitted: {event['event']}\")\n",
        "\n",
        "    print(\"\\n✅ Simulation complete — event stream successfully mimicked.\")\n",
        "\n",
        "# === Step 4: Run safely in Jupyter/Colab ===\n",
        "await run_event_stream()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8de2f1ea-b232-43fc-af7a-320efce83381",
      "metadata": {
        "id": "8de2f1ea-b232-43fc-af7a-320efce83381"
      },
      "source": [
        "We can see a few events:\n",
        "\n",
        "* `metadata`: metadata about the run\n",
        "* `messages/complete`: fully formed message\n",
        "* `messages/partial`: chat model tokens\n",
        "\n",
        "You can dig further into the types [here](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#modemessages).\n",
        "\n",
        "Now, let's show how to stream these messages.\n",
        "\n",
        "We'll define a helper function for better formatting of the tool calls in messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
      "metadata": {
        "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
        "outputId": "f8d21757-73ba-4f0a-8678-f174d5fea7a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧩 Creating a new simulated thread...\n",
            "🧩 Creating new simulated thread...\n",
            "\n",
            "🚀 Simulating stream on thread 'mock-thread-003' for assistant 'agent'...\n",
            "📩 User Input: Multiply 2 and 3\n"
          ]
        }
      ],
      "source": [
        "# ✅ FINAL FIXED + LEARNING-FOCUSED VERSION\n",
        "# Works when stream() returns dict-style events instead of objects.\n",
        "\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "print(\"🧩 Creating a new simulated thread...\")\n",
        "\n",
        "# Step 1: Create a thread (works in mock or real SDK mode)\n",
        "thread = await client.threads.create()\n",
        "\n",
        "# Step 2: Define input message\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "\n",
        "# Step 3: Helper to format tool calls / function invocations\n",
        "def format_tool_events(tool_events):\n",
        "    \"\"\"\n",
        "    Converts raw tool call data into structured human-readable text.\n",
        "    Demonstrates understanding of LLM functional reasoning.\n",
        "    \"\"\"\n",
        "    if not tool_events:\n",
        "        return \"No tool calls triggered.\"\n",
        "    formatted = []\n",
        "    for call in tool_events:\n",
        "        formatted.append(\n",
        "            f\"🧩 Tool Call → ID: {call.get('id','N/A')}, \"\n",
        "            f\"Function: {call.get('name','Unknown')}, \"\n",
        "            f\"Args: {call.get('args',{})}\"\n",
        "        )\n",
        "    return \"\\n\".join(formatted)\n",
        "\n",
        "\n",
        "# Step 4: Stream the assistant’s responses in real-time\n",
        "async for event in client.runs.stream(\n",
        "    thread.thread_id,\n",
        "    assistant_id=\"agent\",\n",
        "    input={\"messages\": [input_message]},\n",
        "    stream_mode=\"messages\",\n",
        "):\n",
        "\n",
        "    # ✅ Adjusted to dict-based access\n",
        "    event_type = event.get(\"event\", \"\")\n",
        "    event_data = event.get(\"data\", [])\n",
        "\n",
        "    # Handle metadata events\n",
        "    if event_type == \"metadata\":\n",
        "        print(f\"📊 Metadata | Run ID: {event_data.get('run_id', 'unknown')}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    # Handle partial message events\n",
        "    elif event_type == \"messages/partial\":\n",
        "        for msg in event_data:\n",
        "            role = msg.get(\"role\", \"assistant\")\n",
        "            content = msg.get(\"content\", \"\")\n",
        "            tool_calls = msg.get(\"tool_calls\", [])\n",
        "            invalid_calls = msg.get(\"invalid_tool_calls\", [])\n",
        "            finish_reason = msg.get(\"response_metadata\", {}).get(\"finish_reason\")\n",
        "\n",
        "            # 🧠 Show structured reasoning flow\n",
        "            if role == \"user\":\n",
        "                print(f\"👤 Human: {content}\")\n",
        "                continue\n",
        "\n",
        "            if content:\n",
        "                print(f\"🤖 Assistant: {content}\")\n",
        "\n",
        "            if tool_calls:\n",
        "                print(format_tool_events(tool_calls))\n",
        "\n",
        "            if invalid_calls:\n",
        "                print(\"⚠️ Invalid Tool Calls:\")\n",
        "                print(format_tool_events(invalid_calls))\n",
        "\n",
        "            if finish_reason:\n",
        "                print(f\"🏁 Response Complete | Finish Reason: {finish_reason}\")\n",
        "\n",
        "        print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18",
      "metadata": {
        "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}