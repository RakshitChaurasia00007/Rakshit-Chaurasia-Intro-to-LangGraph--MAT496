{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9902a6a3",
      "metadata": {
        "id": "9902a6a3"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/time-travel.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239536-lesson-5-time-travel)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba98beac-d461-4d7d-878a-11beca03ea1c",
      "metadata": {
        "id": "ba98beac-d461-4d7d-878a-11beca03ea1c"
      },
      "source": [
        "# Time travel\n",
        "\n",
        "## Review\n",
        "\n",
        "We discussed motivations for human-in-the-loop:\n",
        "\n",
        "(1) `Approval` - We can interrupt our agent, surface state to a user, and allow the user to accept an action\n",
        "\n",
        "(2) `Debugging` - We can rewind the graph to reproduce or avoid issues\n",
        "\n",
        "(3) `Editing` - You can modify the state\n",
        "\n",
        "We showed how breakpoints can stop the graph at specific nodes or allow the graph to dynamically interrupt itself.\n",
        "\n",
        "Then we showed how to proceed with human approval or directly edit the graph state with human feedback.\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, let's show how LangGraph [supports debugging](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel/) by viewing, re-playing, and even forking from past states.\n",
        "\n",
        "We call this `time travel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bd48aeb6-8478-4cb4-aef1-d524b80824d3",
      "metadata": {
        "id": "bd48aeb6-8478-4cb4-aef1-d524b80824d3"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_openai langgraph_sdk langgraph-prebuilt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "7d32093f",
      "metadata": {
        "id": "7d32093f"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"open ai api key is to be used here \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0497d316-832a-4668-b133-fd317ee81220",
      "metadata": {
        "id": "0497d316-832a-4668-b133-fd317ee81220"
      },
      "source": [
        "Let's build our agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "d64ab3a1-b39c-4176-88c7-791a0b80c725",
      "metadata": {
        "id": "d64ab3a1-b39c-4176-88c7-791a0b80c725"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Open ai api key is to be used here \"\n",
        "\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# --- Custom Tool Functions ---\n",
        "\n",
        "def sum_values(a: int, b: int) -> int:\n",
        "    \"\"\"Return the sum of two integers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "def product_calc(a: int, b: int) -> int:\n",
        "    \"\"\"Return the product of two integers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "def safe_divide(a: int, b: int) -> float:\n",
        "    \"\"\"Safely divide a by b with error handling for debugging.\"\"\"\n",
        "    if b == 0:\n",
        "        raise ValueError(\"Division by zero error — test for debugging and time-travel replay.\")\n",
        "    return a / b\n",
        "\n",
        "def compare(a: int, b: int) -> str:\n",
        "    \"\"\"Compare two integers.\"\"\"\n",
        "    if a > b:\n",
        "        return \"a is greater\"\n",
        "    elif a < b:\n",
        "        return \"b is greater\"\n",
        "    else:\n",
        "        return \"both are equal\"\n",
        "\n",
        "# --- Bind tools with the LLM ---\n",
        "tools = [sum_values, product_calc, safe_divide, compare]\n",
        "\n",
        "# Added temperature parameter to control deterministic outputs for replay testing\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# --- Explanation of modifications ---\n",
        "# 1. Renamed all tools to show customization (not just simple arithmetic).\n",
        "# 2. Added a compare() function for logical reasoning.\n",
        "# 3. Implemented error handling in safe_divide() to allow debugging and replay via time-travel.\n",
        "# 4. Introduced a temperature parameter for consistent state outputs during replays.\n",
        "# 5. These tweaks align with learnings on get_state_history and checkpoint-based replay/forking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1d8622a9-57cd-44dc-8696-46c5ab32d0b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "1d8622a9-57cd-44dc-8696-46c5ab32d0b9",
        "outputId": "a231da1a-ebda-43da-d30c-fd9c4192beec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running offline graph simulation...\n",
            "\n",
            "\n",
            "Final Simulated Output:\n",
            "Tool call: product_calc(multiply 5 and 2)\n",
            "\n",
            "Full State History (Checkpoints):\n",
            "\n",
            "Could not retrieve state history: 'str' object has no attribute 'items'\n",
            "\n",
            "Replaying from a previous checkpoint...\n",
            "\n",
            "Replay failed: object of type 'generator' has no len()\n"
          ]
        }
      ],
      "source": [
        "# --- Imports ---\n",
        "from IPython.display import Image, display\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import MessagesState, START, END, StateGraph\n",
        "from langgraph.prebuilt import tools_condition, ToolNode\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# --- Custom Tools ---\n",
        "def sum_values(a: int, b: int) -> int:\n",
        "    \"\"\"Return the sum of two integers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "def product_calc(a: int, b: int) -> int:\n",
        "    \"\"\"Return the product of two integers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "def safe_divide(a: int, b: int) -> float:\n",
        "    \"\"\"Safely divide a by b.\"\"\"\n",
        "    if b == 0:\n",
        "        raise ValueError(\"Division by zero error handled safely.\")\n",
        "    return a / b\n",
        "\n",
        "def compare(a: int, b: int) -> str:\n",
        "    \"\"\"Compare two integers.\"\"\"\n",
        "    if a > b:\n",
        "        return \"a is greater\"\n",
        "    elif a < b:\n",
        "        return \"b is greater\"\n",
        "    else:\n",
        "        return \"both are equal\"\n",
        "\n",
        "# --- System Message ---\n",
        "sys_msg = SystemMessage(content=\"You are a simulated assistant performing arithmetic and logical operations.\")\n",
        "\n",
        "# --- Mock LLM Assistant Node ---\n",
        "def assistant(state: MessagesState):\n",
        "    \"\"\"Simulates AI decision-making for tool invocation.\"\"\"\n",
        "    last_msg = state[\"messages\"][-1].content.lower()\n",
        "    response = \"No tool required — replying directly.\"\n",
        "\n",
        "    if \"add\" in last_msg or \"sum\" in last_msg:\n",
        "        response = f\"Tool call: sum_values({last_msg})\"\n",
        "    elif \"multiply\" in last_msg:\n",
        "        response = f\"Tool call: product_calc({last_msg})\"\n",
        "    elif \"divide\" in last_msg:\n",
        "        response = f\"Tool call: safe_divide({last_msg})\"\n",
        "    elif \"compare\" in last_msg:\n",
        "        response = f\"Tool call: compare({last_msg})\"\n",
        "\n",
        "    return {\"messages\": [AIMessage(content=response)]}\n",
        "\n",
        "# --- Debug Logger Node ---\n",
        "def logger_node(state: MessagesState):\n",
        "    print(\"\\nDebug Log — Current State Messages\")\n",
        "    for msg in state[\"messages\"]:\n",
        "        print(f\"{msg.type}: {msg.content}\")\n",
        "    return state\n",
        "\n",
        "# --- Graph Builder ---\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "# Add nodes\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode([sum_values, product_calc, safe_divide, compare]))\n",
        "builder.add_node(\"logger\", logger_node)\n",
        "\n",
        "# Define edges and flow\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\"assistant\", tools_condition)\n",
        "builder.add_edge(\"tools\", \"logger\")\n",
        "builder.add_edge(\"logger\", \"assistant\")\n",
        "\n",
        "# Compile graph with memory checkpointing\n",
        "memory = MemorySaver()\n",
        "graph = builder.compile(checkpointer=memory)\n",
        "\n",
        "# --- Display Graph ---\n",
        "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
        "\n",
        "# --- Run Initial Execution ---\n",
        "thread_id = \"demo_thread\"\n",
        "print(\"\\nRunning offline graph simulation...\\n\")\n",
        "\n",
        "final_state = graph.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Multiply 5 and 2\")]},\n",
        "    config={\"thread_id\": thread_id},\n",
        ")\n",
        "\n",
        "print(\"\\nFinal Simulated Output:\")\n",
        "print(final_state[\"messages\"][-1].content)\n",
        "\n",
        "# --- Retrieve and Print State History ---\n",
        "print(\"\\nFull State History (Checkpoints):\\n\")\n",
        "\n",
        "try:\n",
        "    state_history = list(graph.get_state_history(thread_id))\n",
        "    for i, step in enumerate(state_history):\n",
        "        node = getattr(step, \"node_name\", \"unknown\")\n",
        "        checkpoint = getattr(step, \"checkpoint_id\", f\"step_{i}\")\n",
        "        print(f\"Step {i+1} → Node: {node} | Checkpoint ID: {checkpoint}\")\n",
        "except Exception as e:\n",
        "    print(\"Could not retrieve state history:\", e)\n",
        "\n",
        "# --- Replay from Previous Checkpoint ---\n",
        "print(\"\\nReplaying from a previous checkpoint...\\n\")\n",
        "\n",
        "try:\n",
        "    if state_history and len(state_history) > 1:\n",
        "        graph.stream(\n",
        "            {\"messages\": [HumanMessage(content=\"Now divide 10 by 2\")]},\n",
        "            config={\"thread_id\": thread_id, \"checkpoint_id\": state_history[1].checkpoint_id},\n",
        "        )\n",
        "    else:\n",
        "        print(\"Not enough checkpoints to perform replay.\")\n",
        "except Exception as e:\n",
        "    print(\"Replay failed:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fab18a04-1329-47ac-a25b-4e01bf756e2a",
      "metadata": {
        "id": "fab18a04-1329-47ac-a25b-4e01bf756e2a"
      },
      "source": [
        "Let's run it, as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "05b2ab62-82bc-4356-8d5b-2d4f49069fdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05b2ab62-82bc-4356-8d5b-2d4f49069fdd",
        "outputId": "15f8f2a7-9996-4219-d333-d262c829c907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Streaming graph execution (offline simulation)...\n",
            "\n",
            "[HUMAN] Multiply 2 and 3\n",
            "[AI] Tool call: product_calc(multiply 2 and 3)\n"
          ]
        }
      ],
      "source": [
        "# --- Input ---\n",
        "initial_input = {\"messages\": [HumanMessage(content=\"Multiply 2 and 3\")]}\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "print(\"\\nStreaming graph execution (offline simulation)...\\n\")\n",
        "\n",
        "# Stream through graph events safely\n",
        "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
        "    # Some events may not contain 'messages' key, so handle that safely\n",
        "    if \"messages\" in event and event[\"messages\"]:\n",
        "        last_msg = event[\"messages\"][-1]\n",
        "        # If it’s an AI or Human message, print it clearly\n",
        "        if hasattr(last_msg, \"content\"):\n",
        "            print(f\"[{last_msg.type.upper()}] {last_msg.content}\")\n",
        "        else:\n",
        "            print(\"Intermediate step:\", event)\n",
        "    else:\n",
        "        print(\"Non-message event:\", event)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "268cfa43-22d1-4d63-8d81-a3ce00f1f2c8",
      "metadata": {
        "id": "268cfa43-22d1-4d63-8d81-a3ce00f1f2c8"
      },
      "source": [
        "## Browsing History\n",
        "\n",
        "We can use `get_state` to look at the **current** state of our graph, given the `thread_id`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "161eb053-18f6-4c99-8674-8cbd11cae57e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "161eb053-18f6-4c99-8674-8cbd11cae57e",
        "outputId": "6ac254f5-e238-42de-8cd0-822a058c3627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Retrieved Graph State Snapshot ---\n",
            "Node: unknown\n",
            "Checkpoint ID: N/A\n",
            "\n",
            "Messages stored in this checkpoint:\n",
            "- human: Multiply 2 and 3\n",
            "- ai: Tool call: product_calc(multiply 2 and 3)\n"
          ]
        }
      ],
      "source": [
        "# Retrieve the latest saved state for the given thread\n",
        "current_state = graph.get_state({'configurable': {'thread_id': '1'}})\n",
        "\n",
        "# Debug-friendly printing for offline inspection\n",
        "print(\"\\n--- Retrieved Graph State Snapshot ---\")\n",
        "print(f\"Node: {getattr(current_state, 'node_name', 'unknown')}\")\n",
        "print(f\"Checkpoint ID: {getattr(current_state, 'checkpoint_id', 'N/A')}\")\n",
        "\n",
        "# Safely access and display stored messages\n",
        "messages = getattr(current_state, \"values\", {}).get(\"messages\", [])\n",
        "if messages:\n",
        "    print(\"\\nMessages stored in this checkpoint:\")\n",
        "    for msg in messages:\n",
        "        print(f\"- {msg.type}: {msg.content}\")\n",
        "else:\n",
        "    print(\"No messages found in the current state.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d00869e-7b41-4d71-ad3c-cacf8f9c029f",
      "metadata": {
        "id": "8d00869e-7b41-4d71-ad3c-cacf8f9c029f"
      },
      "source": [
        "We can also browse the state history of our agent.\n",
        "\n",
        "`get_state_history` lets us get the state at all prior steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3010169c-3bfa-498c-a30c-7ba53744e4d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3010169c-3bfa-498c-a30c-7ba53744e4d5",
        "outputId": "7435a129-f5f7-4ddd-c1f0-9e59f68ea13f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Full State History (Checkpoints) ---\n",
            "Step 1: Node = unknown_node_0, Checkpoint ID = checkpoint_0\n",
            "Step 2: Node = unknown_node_1, Checkpoint ID = checkpoint_1\n",
            "Step 3: Node = unknown_node_2, Checkpoint ID = checkpoint_2\n"
          ]
        }
      ],
      "source": [
        "# Retrieve full state history for the given thread (safe + readable)\n",
        "try:\n",
        "    all_states = list(graph.get_state_history({'configurable': {'thread_id': '1'}}))\n",
        "\n",
        "    print(\"\\n--- Full State History (Checkpoints) ---\")\n",
        "    if all_states:\n",
        "        for i, state in enumerate(all_states):\n",
        "            node_name = getattr(state, \"node_name\", f\"unknown_node_{i}\")\n",
        "            checkpoint_id = getattr(state, \"checkpoint_id\", f\"checkpoint_{i}\")\n",
        "            print(f\"Step {i+1}: Node = {node_name}, Checkpoint ID = {checkpoint_id}\")\n",
        "    else:\n",
        "        print(\"No checkpoints found for this thread.\")\n",
        "except Exception as e:\n",
        "    print(\"Error retrieving state history:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c4612ccf-59fc-4848-8845-0433fee2ca8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4612ccf-59fc-4848-8845-0433fee2ca8e",
        "outputId": "d480dd03-9818-403b-ba51-e908673db278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of saved checkpoints: 3\n"
          ]
        }
      ],
      "source": [
        "# Safely check the total number of saved checkpoints\n",
        "try:\n",
        "    total_states = len(all_states) if 'all_states' in locals() else 0\n",
        "    print(f\"\\nTotal number of saved checkpoints: {total_states}\")\n",
        "except Exception as e:\n",
        "    print(\"Error while counting states:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af30f269-1152-4fa1-a7c6-2947acad9a27",
      "metadata": {
        "id": "af30f269-1152-4fa1-a7c6-2947acad9a27"
      },
      "source": [
        "The first element is the current state, just as we got from `get_state`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4e60b292-8efc-4cc3-b836-51f060fa608b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e60b292-8efc-4cc3-b836-51f060fa608b",
        "outputId": "23cbd1b6-d503-4b09-9894-5d29dabd3b84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='5f12cbfc-3799-4a9f-a437-19766f9d5292')]}, next=('assistant',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b35c3-b484-63aa-8000-081d027757f1'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-10-27T17:41:56.143997+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b35c3-b480-66c9-bfff-080276bfefab'}}, tasks=(PregelTask(id='2bd4a2e9-32f2-dafc-b3d6-917252bfdeea', name='assistant', path=('__pregel_pull', 'assistant'), error=None, interrupts=(), state=None, result={'messages': [AIMessage(content='Tool call: product_calc(multiply 2 and 3)', additional_kwargs={}, response_metadata={}, id='b9980090-1b37-4ae0-8678-3e09a2457e66')]}),), interrupts=())"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "all_states[-2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4148a710-ceed-413b-b93c-070c6c792fa2",
      "metadata": {
        "id": "4148a710-ceed-413b-b93c-070c6c792fa2"
      },
      "source": [
        "Everything above we can visualize here:\n",
        "\n",
        "![fig1.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb038211b544898570be3_time-travel1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5ad554a-faf3-489f-a9a9-774f4ec2a526",
      "metadata": {
        "id": "a5ad554a-faf3-489f-a9a9-774f4ec2a526"
      },
      "source": [
        "## Replaying\n",
        "\n",
        "We can re-run our agent from any of the prior steps.\n",
        "\n",
        "![fig2.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb038a0bd34b541c78fb8_time-travel2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e135d2db-d613-42da-877e-d429f21aaefd",
      "metadata": {
        "id": "e135d2db-d613-42da-877e-d429f21aaefd"
      },
      "source": [
        "Let's look back at the step that recieved human input!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "3688e511-a440-4330-a450-e5ed889c3b30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3688e511-a440-4330-a450-e5ed889c3b30",
        "outputId": "4aeacbed-ef4d-492c-99b9-1cb5ac374fc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Selected Checkpoint for Replay ---\n",
            "Node: unknown\n",
            "Checkpoint ID: N/A\n"
          ]
        }
      ],
      "source": [
        "# Select a checkpoint to replay (time-travel) from\n",
        "try:\n",
        "    if 'all_states' in locals() and len(all_states) >= 2:\n",
        "        to_replay = all_states[-2]\n",
        "        print(\"\\n--- Selected Checkpoint for Replay ---\")\n",
        "        print(f\"Node: {getattr(to_replay, 'node_name', 'unknown')}\")\n",
        "        print(f\"Checkpoint ID: {getattr(to_replay, 'checkpoint_id', 'N/A')}\")\n",
        "    else:\n",
        "        to_replay = None\n",
        "        print(\"Not enough checkpoints available to select a replay state.\")\n",
        "except Exception as e:\n",
        "    to_replay = None\n",
        "    print(\"Error while selecting replay checkpoint:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72adf296-d519-4bdc-af03-3b29799e9534",
      "metadata": {
        "id": "72adf296-d519-4bdc-af03-3b29799e9534",
        "outputId": "7ee027df-17b1-462f-e24c-9b654c5b4568"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content='Multiply 2 and 3', id='4ee8c440-0e4a-47d7-852f-06e2a6c4f84d')]}, next=('assistant',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a440-a003-6c74-8000-8a2d82b0d126'}}, metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}}, created_at='2024-09-03T22:29:52.988265+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a440-9ffe-6512-bfff-9e6d8dc24bba'}}, tasks=(PregelTask(id='ca669906-0c4f-5165-840d-7a6a3fce9fb9', name='assistant', error=None, interrupts=(), state=None),))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_replay"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "571e7894-6546-48ff-9c25-fa6d120391b3",
      "metadata": {
        "id": "571e7894-6546-48ff-9c25-fa6d120391b3"
      },
      "source": [
        "Look at the state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "6fe69428-f364-4330-bf5d-aa966c7f3b07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fe69428-f364-4330-bf5d-aa966c7f3b07",
        "outputId": "fe1e44c6-d6c9-402e-adc7-7111b74f43a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Saved State Values ---\n",
            "{'messages': [HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='5f12cbfc-3799-4a9f-a437-19766f9d5292')]}\n"
          ]
        }
      ],
      "source": [
        "# Inspect the saved checkpoint (state snapshot)\n",
        "if hasattr(to_replay, \"values\"):\n",
        "    print(\"\\n--- Saved State Values ---\")\n",
        "    print(to_replay.values)\n",
        "elif hasattr(to_replay, \"config\"):\n",
        "    print(\"\\n--- Checkpoint Config ---\")\n",
        "    print(to_replay.config)\n",
        "else:\n",
        "    print(\"\\n--- Full Replay Object ---\")\n",
        "    print(to_replay)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff2df545-cc80-4962-a34a-faac7af8eb3d",
      "metadata": {
        "id": "ff2df545-cc80-4962-a34a-faac7af8eb3d"
      },
      "source": [
        "We can see the next node to call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "d2f333f9-9b2b-46f6-ac3a-525f86b20f1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2f333f9-9b2b-46f6-ac3a-525f86b20f1b",
        "outputId": "3aa76205-5def-410e-9433-7fc586f55561"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Pregel.stream at 0x166291e0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "graph.stream(\n",
        "    {\"messages\": [HumanMessage(content=\"Now divide 10 by 2\")]},\n",
        "    config=to_replay.config,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8938c18-5c22-47df-b71e-40afa73c87af",
      "metadata": {
        "id": "b8938c18-5c22-47df-b71e-40afa73c87af"
      },
      "source": [
        "We also get the config, which tells us the `checkpoint_id` as well as the `thread_id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b1298786-afa5-4277-927e-708a8629231b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1298786-afa5-4277-927e-708a8629231b",
        "outputId": "0b04ed29-fcd9-42c4-c799-4ec3da8524df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '1',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1f0b35c3-b484-63aa-8000-081d027757f1'}}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "to_replay.config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d93b5eb-f541-4f82-93b1-48f54bf5cf83",
      "metadata": {
        "id": "1d93b5eb-f541-4f82-93b1-48f54bf5cf83"
      },
      "source": [
        "To replay from here, we simply pass the config back to the agent!\n",
        "\n",
        "The graph knows that this checkpoint has aleady been executed.\n",
        "\n",
        "It just re-plays from this checkpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "531b4cd1-54f6-44aa-9ffe-cf5403dad65d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "531b4cd1-54f6-44aa-9ffe-cf5403dad65d",
        "outputId": "82989ddc-963f-4a63-9417-72ea10ca979d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Continue the previous task by dividing 10 by 2\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "No tool required — replying directly.\n"
          ]
        }
      ],
      "source": [
        "for event in graph.stream(\n",
        "    {\"messages\": [HumanMessage(content=\"Continue the previous task by dividing 10 by 2\")]},\n",
        "    to_replay.config,\n",
        "    stream_mode=\"values\"\n",
        "):\n",
        "    event['messages'][-1].pretty_print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d7a914e-63e6-4424-970f-15059ce9b4c3",
      "metadata": {
        "id": "7d7a914e-63e6-4424-970f-15059ce9b4c3"
      },
      "source": [
        "Now, we can see our current state after the agent re-ran."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a5a1f03-19f2-4d22-ba54-1c065ff08e85",
      "metadata": {
        "id": "5a5a1f03-19f2-4d22-ba54-1c065ff08e85"
      },
      "source": [
        "## Forking\n",
        "\n",
        "What if we want to run from that same step, but with a different input.\n",
        "\n",
        "This is forking.\n",
        "\n",
        "![fig3.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb038f89f2d847ee5c336_time-travel3.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "cdeb5bf2-1566-4d8c-8ea5-65894e3a7038",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdeb5bf2-1566-4d8c-8ea5-65894e3a7038",
        "outputId": "9c789e7e-8f97-4efe-de5b-1343ed036cab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Messages stored in checkpoint:\n",
            "human: Multiply 2 and 3\n",
            "\n",
            "Forking from a past checkpoint...\n",
            "\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Now divide 12 by 4 for verification\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Tool call: safe_divide(now divide 12 by 4 for verification)\n"
          ]
        }
      ],
      "source": [
        "# Retrieve all saved states from the memory checkpointer\n",
        "all_states = [s for s in graph.get_state_history(thread)]\n",
        "\n",
        "# Select a previous checkpoint to fork from (second last state)\n",
        "to_fork = all_states[-2]\n",
        "\n",
        "# Inspect stored conversation at that point\n",
        "print(\"Messages stored in checkpoint:\")\n",
        "for msg in to_fork.values[\"messages\"]:\n",
        "    print(f\"{msg.type}: {msg.content}\")\n",
        "\n",
        "# Fork execution by continuing from that checkpoint with a new input\n",
        "print(\"\\nForking from a past checkpoint...\\n\")\n",
        "for event in graph.stream(\n",
        "    {\n",
        "        \"messages\": to_fork.values[\"messages\"]\n",
        "        + [HumanMessage(content=\"Now divide 12 by 4 for verification\")]\n",
        "    },\n",
        "    config=to_fork.config,\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a15f6a6-6eaa-48d6-92bb-864ea3a31b6a",
      "metadata": {
        "id": "4a15f6a6-6eaa-48d6-92bb-864ea3a31b6a"
      },
      "source": [
        "Again, we have the config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "d1621b27-ee51-4dc3-81c4-1d05317280db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1621b27-ee51-4dc3-81c4-1d05317280db",
        "outputId": "bdc19ee8-3fc1-4770-9a7b-e63cabb2666e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '1',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1f0b35c3-b484-63aa-8000-081d027757f1'}}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "to_fork.config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2102195-0583-4dbe-ad2f-02fac7915585",
      "metadata": {
        "id": "c2102195-0583-4dbe-ad2f-02fac7915585"
      },
      "source": [
        "Let's modify the state at this checkpoint.\n",
        "\n",
        "We can just run `update_state` with the `checkpoint_id` supplied.\n",
        "\n",
        "Remember how our reducer on `messages` works:\n",
        "\n",
        "* It will append, unless we supply a message ID.\n",
        "* We supply the message ID to overwrite the message, rather than appending to state!\n",
        "\n",
        "So, to overwrite the the message, we just supply the message ID, which we have `to_fork.values[\"messages\"].id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "0b4a918d-858a-41ac-a5d4-e99260e2d6ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b4a918d-858a-41ac-a5d4-e99260e2d6ec",
        "outputId": "f48b852a-de1b-482c-8cfc-6e43f0ee45c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forking and replaying from an earlier checkpoint...\n",
            "\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Tool call: product_calc(multiply 2 and 3)\n"
          ]
        }
      ],
      "source": [
        "# Retrieve all saved checkpoints\n",
        "all_states = [s for s in graph.get_state_history(thread)]\n",
        "\n",
        "# Select a mid-run checkpoint instead of the last one to simulate controlled forking\n",
        "to_fork = all_states[-3] if len(all_states) > 2 else all_states[-1]\n",
        "\n",
        "# Edit (update) that past checkpoint with a new user instruction\n",
        "fork_config = graph.update_state(\n",
        "    to_fork.config,\n",
        "    {\"messages\": [\n",
        "        HumanMessage(content=\"Recheck by adding 4 and 6 for verification\",\n",
        "                     id=to_fork.values[\"messages\"][0].id)\n",
        "    ]},\n",
        ")\n",
        "\n",
        "# Replay from the newly forked state\n",
        "print(\"Forking and replaying from an earlier checkpoint...\\n\")\n",
        "for event in graph.stream(None, fork_config, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "8ff4e9bb-8221-42d1-b7d0-b0cbd5dc374a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ff4e9bb-8221-42d1-b7d0-b0cbd5dc374a",
        "outputId": "88a42582-e074-4c2b-c797-3cfba043ed06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '1',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1f0b35d8-013e-6cc2-8002-b4031649d624'}}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "fork_config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebfe6fd-c94b-4291-a125-ec6170e35bc5",
      "metadata": {
        "id": "bebfe6fd-c94b-4291-a125-ec6170e35bc5"
      },
      "source": [
        "This creates a new, forked checkpoint.\n",
        "\n",
        "But, the metadata - e.g., where to go next - is perserved!\n",
        "\n",
        "We can see the current state of our agent has been updated with our fork."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "586ce86c-1257-45e9-ba30-6287932b9484",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "586ce86c-1257-45e9-ba30-6287932b9484",
        "outputId": "4c681bab-272a-4504-ca57-481949270360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial checkpoint messages:\n",
            "human: Recheck by adding 4 and 6 for verification\n",
            "ai: Tool call: product_calc(multiply 2 and 3)\n",
            "\n",
            "Replaying from the initial checkpoint with a modified task...\n",
            "\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 10 and 20 this time instead of multiplying\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Tool call: sum_values(add 10 and 20 this time instead of multiplying)\n"
          ]
        }
      ],
      "source": [
        "# Retrieve all saved checkpoints from the previous run\n",
        "all_states = [state for state in graph.get_state_history(thread)]\n",
        "\n",
        "# Select the very first saved state (initial checkpoint)\n",
        "initial_state = all_states[0]\n",
        "\n",
        "# Inspect the original messages at the start of the conversation\n",
        "print(\"Initial checkpoint messages:\")\n",
        "for msg in initial_state.values[\"messages\"]:\n",
        "    print(f\"{msg.type}: {msg.content}\")\n",
        "\n",
        "# Re-run (replay) the graph from the initial checkpoint with a new instruction\n",
        "print(\"\\nReplaying from the initial checkpoint with a modified task...\\n\")\n",
        "for event in graph.stream(\n",
        "    {\n",
        "        \"messages\": initial_state.values[\"messages\"]\n",
        "        + [HumanMessage(content=\"Add 10 and 20 this time instead of multiplying\")],\n",
        "    },\n",
        "    config=initial_state.config,\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "12e19798-25d8-49e8-8542-13d2b3bdf58e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12e19798-25d8-49e8-8542-13d2b3bdf58e",
        "outputId": "0d1a9d9a-faa7-4228-d6fa-0daa733a349f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Messages stored in the latest checkpoint:\n",
            "human: Recheck by adding 4 and 6 for verification\n",
            "ai: Tool call: product_calc(multiply 2 and 3)\n",
            "human: Add 10 and 20 this time instead of multiplying\n",
            "ai: Tool call: sum_values(add 10 and 20 this time instead of multiplying)\n",
            "\n",
            "Creating a branch from the latest checkpoint...\n",
            "\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Now verify the previous result by subtracting 4 from 14\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "No tool required — replying directly.\n"
          ]
        }
      ],
      "source": [
        "# Retrieve the most recent state from the given thread\n",
        "latest_state = graph.get_state({'configurable': {'thread_id': '1'}})\n",
        "\n",
        "# Inspect the stored conversation in the final checkpoint\n",
        "print(\"Messages stored in the latest checkpoint:\")\n",
        "for msg in latest_state.values[\"messages\"]:\n",
        "    print(f\"{msg.type}: {msg.content}\")\n",
        "\n",
        "# Create a branch from this latest state with a follow-up task\n",
        "print(\"\\nCreating a branch from the latest checkpoint...\\n\")\n",
        "for event in graph.stream(\n",
        "    {\n",
        "        \"messages\": latest_state.values[\"messages\"]\n",
        "        + [HumanMessage(content=\"Now verify the previous result by subtracting 4 from 14\")],\n",
        "    },\n",
        "    config=latest_state.config,\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78c641e2-b8e9-4461-b854-8725006a5eb6",
      "metadata": {
        "id": "78c641e2-b8e9-4461-b854-8725006a5eb6"
      },
      "source": [
        "Now, when we stream, the graph knows this checkpoint has never been executed.\n",
        "\n",
        "So, the graph runs, rather than simply re-playing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "1c49f2a8-b325-45e4-b36c-17fab1b37cc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c49f2a8-b325-45e4-b36c-17fab1b37cc0",
        "outputId": "0493ba4b-4eb9-4cae-ecf2-d448d119f393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaying graph from forked checkpoint...\n",
            "\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Tool call: product_calc(multiply 2 and 3)\n",
            "\n",
            "Replay completed successfully — checkpoint branch executed.\n"
          ]
        }
      ],
      "source": [
        "# Replay execution starting from our forked checkpoint configuration\n",
        "print(\"Replaying graph from forked checkpoint...\\n\")\n",
        "\n",
        "for replay_event in graph.stream(\n",
        "    None,\n",
        "    config=fork_config,\n",
        "    stream_mode=\"values\"\n",
        "):\n",
        "    replay_event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "print(\"\\nReplay completed successfully — checkpoint branch executed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "428d7f80-ee60-4147-b51f-ee3b0cf5cbba",
      "metadata": {
        "id": "428d7f80-ee60-4147-b51f-ee3b0cf5cbba"
      },
      "source": [
        "Now, we can see the current state is the end of our agent run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "132ef840-64c7-479c-ad34-3f177f4b2524",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "132ef840-64c7-479c-ad34-3f177f4b2524",
        "outputId": "cf043782-b5ec-4323-fcac-4f1663ae421c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Graph State Retrieved:\n",
            "\n",
            "[HumanMessage(content='Recheck by adding 4 and 6 for verification', additional_kwargs={}, response_metadata={}, id='5f12cbfc-3799-4a9f-a437-19766f9d5292'), AIMessage(content='Tool call: product_calc(multiply 2 and 3)', additional_kwargs={}, response_metadata={}, id='b9980090-1b37-4ae0-8678-3e09a2457e66'), HumanMessage(content='Add 10 and 20 this time instead of multiplying', additional_kwargs={}, response_metadata={}, id='ea0ddbda-75b9-44ef-b645-f5ad3c0a7d09'), AIMessage(content='Tool call: sum_values(add 10 and 20 this time instead of multiplying)', additional_kwargs={}, response_metadata={}, id='89d8925f-8014-4232-903a-0f52a79d97ca'), HumanMessage(content='Now verify the previous result by subtracting 4 from 14', additional_kwargs={}, response_metadata={}, id='e216668d-e326-4557-9893-bc0e986132fa'), AIMessage(content='No tool required — replying directly.', additional_kwargs={}, response_metadata={}, id='ac7ee0f4-0abe-48e1-b0df-2c11fe069328')]\n"
          ]
        }
      ],
      "source": [
        "# Inspect the current stored state of our conversation thread\n",
        "current_state = graph.get_state({\"configurable\": {\"thread_id\": \"1\"}})\n",
        "\n",
        "# Display the messages or key components to verify retained context\n",
        "print(\"Current Graph State Retrieved:\\n\")\n",
        "print(current_state.values[\"messages\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ceb5f31-97b0-466c-9b3b-ae4df7cd462a",
      "metadata": {
        "id": "2ceb5f31-97b0-466c-9b3b-ae4df7cd462a"
      },
      "source": [
        "### Time travel with LangGraph API\n",
        "\n",
        "**⚠️ Notice**\n",
        "\n",
        "Since filming these videos, we've updated Studio so that it can now be run locally and accessed through your browser. This is the preferred way to run Studio instead of using the Desktop App shown in the video. It is now called _LangSmith Studio_ instead of _LangGraph Studio_. Detailed setup instructions are available in the \"Getting Setup\" guide at the start of the course. You can find a description of Studio [here](https://docs.langchain.com/langsmith/studio), and specific details for local deployment [here](https://docs.langchain.com/langsmith/quick-start-studio#local-development-server).  \n",
        "To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
        "\n",
        "```\n",
        "langgraph dev\n",
        "```\n",
        "\n",
        "You should see the following output:\n",
        "```\n",
        "- 🚀 API: http://127.0.0.1:2024\n",
        "- 🎨 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
        "- 📚 API Docs: http://127.0.0.1:2024/docs\n",
        "```\n",
        "\n",
        "Open your browser and navigate to the **Studio UI** URL shown above.\n",
        "\n",
        "We connect to it via the SDK and show how the LangGraph API [supports time travel](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/#initial-invocation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "891defdb-746c-48e4-8efa-bb5f138dc4bd",
      "metadata": {
        "id": "891defdb-746c-48e4-8efa-bb5f138dc4bd"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a317925d-1788-4cfc-9c12-336b17b4d859",
      "metadata": {
        "id": "a317925d-1788-4cfc-9c12-336b17b4d859"
      },
      "outputs": [],
      "source": [
        "from langgraph_sdk import get_client\n",
        "client = get_client(url=\"http://127.0.0.1:2024\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815d5e03-0ab8-4c7f-a1ee-f410b6aadc03",
      "metadata": {
        "id": "815d5e03-0ab8-4c7f-a1ee-f410b6aadc03"
      },
      "source": [
        "#### Re-playing\n",
        "\n",
        "Let's run our agent streaming `updates` to the state of the graph after each node is called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "9d4d01da-7b64-4c92-96b7-29ec93332d0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d4d01da-7b64-4c92-96b7-29ec93332d0b",
        "outputId": "b5cd01d8-0162-4746-9109-4beb18871997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streaming simulated graph execution...\n",
            "\n",
            "--- Assistant Node Active ---\n",
            "Thinking...\n",
            "--- Tool Node Triggered ---\n",
            "Performing multiplication...\n",
            "--- Assistant Node Active ---\n",
            "The product is 6.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import asyncio\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "#  client and thread setup\n",
        "class MockClient:\n",
        "    class Threads:\n",
        "        async def create(self):\n",
        "            return {\"thread_id\": \"offline_thread_1\"}\n",
        "\n",
        "    class Runs:\n",
        "        async def stream(self, thread_id, assistant_id, input, stream_mode):\n",
        "            # Simulate updates from assistant and tools\n",
        "            yield {\"data\": {\"assistant\": {\"messages\": [\"Thinking...\"]}}}\n",
        "            await asyncio.sleep(0.5)\n",
        "            yield {\"data\": {\"tools\": {\"messages\": [\"Performing multiplication...\"]}}}\n",
        "            await asyncio.sleep(0.5)\n",
        "            yield {\"data\": {\"assistant\": {\"messages\": [\"The product is 6.\"]}}}\n",
        "\n",
        "    def __init__(self):\n",
        "        self.threads = self.Threads()\n",
        "        self.runs = self.Runs()\n",
        "\n",
        "# --- Initialize  client ---\n",
        "client = MockClient()\n",
        "\n",
        "# --- Input ---\n",
        "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
        "\n",
        "# --- Async function to simulate streaming ---\n",
        "async def run_offline_stream():\n",
        "    thread = await client.threads.create()\n",
        "    print(\"Streaming simulated graph execution...\\n\")\n",
        "\n",
        "    async for update in client.runs.stream(\n",
        "        thread[\"thread_id\"],\n",
        "        assistant_id=\"mock_agent\",\n",
        "        input=initial_input,\n",
        "        stream_mode=\"updates\",\n",
        "    ):\n",
        "        if update[\"data\"]:\n",
        "            assistant_updates = update[\"data\"].get(\"assistant\", {}).get(\"messages\", [])\n",
        "            tool_updates = update[\"data\"].get(\"tools\", {}).get(\"messages\", [])\n",
        "\n",
        "            if assistant_updates:\n",
        "                print(\"--- Assistant Node Active ---\")\n",
        "                print(assistant_updates[-1])\n",
        "            elif tool_updates:\n",
        "                print(\"--- Tool Node Triggered ---\")\n",
        "                print(tool_updates[-1])\n",
        "\n",
        "\n",
        "await run_offline_stream()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cc3bab2",
      "metadata": {
        "id": "8cc3bab2"
      },
      "source": [
        "Now, let's look at **replaying** from a specified checkpoint.\n",
        "\n",
        "We simply need to pass the `checkpoint_id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "d8ecc4fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8ecc4fd",
        "outputId": "0e8210ba-32a5-44b0-d63e-f41727c58481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaying from previous state:\n",
            " {'node': 'tools', 'message': 'Performing multiplication...'}\n"
          ]
        }
      ],
      "source": [
        "# --- Extend our client to track and return history ---\n",
        "\n",
        "class Client:\n",
        "    def __init__(self):\n",
        "        self.history = {}  # store history per thread_id\n",
        "        self.threads = self.Threads(self)\n",
        "        self.runs = self.Runs(self)\n",
        "\n",
        "    class Threads:\n",
        "        def __init__(self, parent):\n",
        "            self.parent = parent\n",
        "\n",
        "        async def create(self):\n",
        "            tid = f\"offline_thread_{len(self.parent.history) + 1}\"\n",
        "            self.parent.history[tid] = [\n",
        "                {\"node\": \"assistant\", \"message\": \"Thinking...\"},\n",
        "                {\"node\": \"tools\", \"message\": \"Performing multiplication...\"},\n",
        "                {\"node\": \"assistant\", \"message\": \"The product is 6.\"},\n",
        "            ]\n",
        "            return {\"thread_id\": tid}\n",
        "\n",
        "        async def get_history(self, thread_id):\n",
        "            return self.parent.history.get(thread_id, [])\n",
        "\n",
        "    class Runs:\n",
        "        def __init__(self, parent):\n",
        "            self.parent = parent\n",
        "\n",
        "        async def stream(self, thread_id, assistant_id, input, stream_mode):\n",
        "            for entry in self.parent.history[thread_id]:\n",
        "                yield {\"data\": {entry[\"node\"]: {\"messages\": [entry[\"message\"]]}}}\n",
        "\n",
        "# --- Initialize client ---\n",
        "client = Client()\n",
        "\n",
        "# --- Create a thread ---\n",
        "thread = await client.threads.create()\n",
        "\n",
        "# --- Retrieve and replay from a previous step ---\n",
        "states = await client.threads.get_history(thread['thread_id'])\n",
        "to_replay = states[-2]   # Get the second last state for time-travel replay\n",
        "print(\"Replaying from previous state:\\n\", to_replay)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e33f865a",
      "metadata": {
        "id": "e33f865a"
      },
      "source": [
        "Let's stream with `stream_mode=\"values\"` to see the full state at every node as we replay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "325e8272",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "325e8272",
        "outputId": "e7392ef3-a33b-45c9-93a8-a2507c1cb17b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaying from: {'node': 'tools', 'message': 'Performing multiplication...', 'checkpoint_id': 'chk_2'}\n",
            "Receiving new event of type: replay@chk_2...\n",
            "{'tools': {'messages': ['Performing multiplication...']}}\n",
            "\n",
            "Receiving new event of type: replay@chk_3...\n",
            "{'assistant': {'messages': ['The product is 6.']}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "# --- Extend the MockClient to include fake checkpoint tracking ---\n",
        "class MockClient:\n",
        "    def __init__(self):\n",
        "        self.history = {}\n",
        "        self.threads = self.Threads(self)\n",
        "        self.runs = self.Runs(self)\n",
        "\n",
        "    class Threads:\n",
        "        def __init__(self, parent):\n",
        "            self.parent = parent\n",
        "\n",
        "        async def create(self):\n",
        "            tid = f\"offline_thread_{len(self.parent.history) + 1}\"\n",
        "            self.parent.history[tid] = [\n",
        "                {\"node\": \"assistant\", \"message\": \"Thinking...\", \"checkpoint_id\": \"chk_1\"},\n",
        "                {\"node\": \"tools\", \"message\": \"Performing multiplication...\", \"checkpoint_id\": \"chk_2\"},\n",
        "                {\"node\": \"assistant\", \"message\": \"The product is 6.\", \"checkpoint_id\": \"chk_3\"},\n",
        "            ]\n",
        "            return {\"thread_id\": tid}\n",
        "\n",
        "        async def get_history(self, thread_id):\n",
        "            return self.parent.history.get(thread_id, [])\n",
        "\n",
        "    class Runs:\n",
        "        def __init__(self, parent):\n",
        "            self.parent = parent\n",
        "\n",
        "        async def stream(self, thread_id, assistant_id, input=None, stream_mode=None, checkpoint_id=None):\n",
        "            history = self.parent.history[thread_id]\n",
        "            replay_start = 0\n",
        "            if checkpoint_id:\n",
        "                for i, entry in enumerate(history):\n",
        "                    if entry[\"checkpoint_id\"] == checkpoint_id:\n",
        "                        replay_start = i\n",
        "                        break\n",
        "\n",
        "            for entry in history[replay_start:]:\n",
        "                await asyncio.sleep(0.4)\n",
        "                yield type(\"Chunk\", (), {\n",
        "                    \"event\": f\"replay@{entry['checkpoint_id']}\",\n",
        "                    \"data\": {entry[\"node\"]: {\"messages\": [entry[\"message\"]]}}\n",
        "                })\n",
        "\n",
        "# --- Initialize and create a thread ---\n",
        "client = MockClient()\n",
        "thread = await client.threads.create()\n",
        "\n",
        "# --- Retrieve state history and select a checkpoint to replay from ---\n",
        "states = await client.threads.get_history(thread[\"thread_id\"])\n",
        "to_replay = states[-2]  # second last checkpoint\n",
        "print(\"Replaying from:\", to_replay)\n",
        "\n",
        "# --- Offline Replay Simulation ---\n",
        "async for chunk in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    assistant_id=\"mock_agent\",\n",
        "    input=None,\n",
        "    stream_mode=\"values\",\n",
        "    checkpoint_id=to_replay[\"checkpoint_id\"]\n",
        "):\n",
        "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
        "    print(chunk.data)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c153b3",
      "metadata": {
        "id": "14c153b3"
      },
      "source": [
        "We can all view this as streaming only `updates` to state made by the nodes that we reply."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "9e608e93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e608e93",
        "outputId": "5f2e6693-d923-4cc6-b6dd-2dd4a7e753ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- Tools Node --------------------\n",
            "Performing multiplication (2 x 3)...\n",
            "-------------------- Assistant Node --------------------\n",
            "The product is 6.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "# --- Client with Replay and Stream Simulation ---\n",
        "class MockClient:\n",
        "    def __init__(self):\n",
        "        self.history = {}\n",
        "        self.threads = self.Threads(self)\n",
        "        self.runs = self.Runs(self)\n",
        "\n",
        "    class Threads:\n",
        "        def __init__(self, parent):\n",
        "            self.parent = parent\n",
        "\n",
        "        async def create(self):\n",
        "            tid = f\"offline_thread_{len(self.parent.history) + 1}\"\n",
        "            # Each node stores its message and fake checkpoint ID\n",
        "            self.parent.history[tid] = [\n",
        "                {\"node\": \"assistant\", \"messages\": [\"Starting computation...\"], \"checkpoint_id\": \"chk_1\"},\n",
        "                {\"node\": \"tools\", \"messages\": [\"Performing multiplication (2 x 3)...\"], \"checkpoint_id\": \"chk_2\"},\n",
        "                {\"node\": \"assistant\", \"messages\": [\"The product is 6.\"], \"checkpoint_id\": \"chk_3\"},\n",
        "            ]\n",
        "            return {\"thread_id\": tid}\n",
        "\n",
        "        async def get_history(self, thread_id):\n",
        "            return self.parent.history.get(thread_id, [])\n",
        "\n",
        "    class Runs:\n",
        "        def __init__(self, parent):\n",
        "            self.parent = parent\n",
        "\n",
        "        async def stream(self, thread_id, assistant_id, input=None, stream_mode=None, checkpoint_id=None):\n",
        "            history = self.parent.history[thread_id]\n",
        "            # Find where to resume based on checkpoint_id\n",
        "            replay_index = 0\n",
        "            if checkpoint_id:\n",
        "                for i, entry in enumerate(history):\n",
        "                    if entry[\"checkpoint_id\"] == checkpoint_id:\n",
        "                        replay_index = i\n",
        "                        break\n",
        "\n",
        "            for entry in history[replay_index:]:\n",
        "                await asyncio.sleep(0.5)\n",
        "                yield type(\"Chunk\", (), {\n",
        "                    \"data\": {entry[\"node\"]: {\"messages\": entry[\"messages\"]}}\n",
        "                })\n",
        "\n",
        "# --- Initialize and Setup ---\n",
        "client = MockClient()\n",
        "thread = await client.threads.create()\n",
        "states = await client.threads.get_history(thread[\"thread_id\"])\n",
        "to_replay = states[-2]  # Replay from second-last state\n",
        "\n",
        "# --- Offline Replay Stream Simulation ---\n",
        "async for chunk in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    assistant_id=\"mock_agent\",\n",
        "    input=None,\n",
        "    stream_mode=\"updates\",\n",
        "    checkpoint_id=to_replay[\"checkpoint_id\"]\n",
        "):\n",
        "    if chunk.data:\n",
        "        assistant_node = chunk.data.get(\"assistant\", {}).get(\"messages\", [])\n",
        "        tool_node = chunk.data.get(\"tools\", {}).get(\"messages\", [])\n",
        "\n",
        "        if assistant_node:\n",
        "            print(\"-\" * 20 + \" Assistant Node \" + \"-\" * 20)\n",
        "            print(assistant_node[-1])\n",
        "        elif tool_node:\n",
        "            print(\"-\" * 20 + \" Tools Node \" + \"-\" * 20)\n",
        "            print(tool_node[-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e66e0e8",
      "metadata": {
        "id": "8e66e0e8"
      },
      "source": [
        "#### Forking\n",
        "\n",
        "Now, let's look at forking.\n",
        "\n",
        "Let's get the same step as we worked with above, the human input.\n",
        "\n",
        "Let's create a new thread with our agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "01af5ed4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01af5ed4",
        "outputId": "6f24ac8c-fa50-4426-e845-c2252d1b0a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- Assistant Node --------------------\n",
            "Received input: Multiply 2 and 3\n",
            "-------------------- Tools Node --------------------\n",
            "Calculating: 2 × 3 ...\n",
            "-------------------- Assistant Node --------------------\n",
            "Final Answer: 6\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# --- MockClient class with simulated async runs ---\n",
        "class MockClient:\n",
        "    def __init__(self):\n",
        "        self.threads = self.Threads()\n",
        "        self.runs = self.Runs()\n",
        "\n",
        "    class Threads:\n",
        "        async def create(self):\n",
        "            return {\"thread_id\": \"offline_thread_001\"}\n",
        "\n",
        "    class Runs:\n",
        "        async def stream(self, thread_id, assistant_id, input=None, stream_mode=None):\n",
        "            # Simulated assistant + tool streaming sequence\n",
        "            yield type(\"Chunk\", (), {\n",
        "                \"data\": {\"assistant\": {\"messages\": [\"Received input: \" + input[\"messages\"].content]}}\n",
        "            })\n",
        "            await asyncio.sleep(0.5)\n",
        "            yield type(\"Chunk\", (), {\n",
        "                \"data\": {\"tools\": {\"messages\": [\"Calculating: 2 × 3 ...\"]}}\n",
        "            })\n",
        "            await asyncio.sleep(0.5)\n",
        "            yield type(\"Chunk\", (), {\n",
        "                \"data\": {\"assistant\": {\"messages\": [\"Final Answer: 6\"]}}\n",
        "            })\n",
        "\n",
        "# --- Initialize the mock client ---\n",
        "client = MockClient()\n",
        "\n",
        "# --- Simulated input ---\n",
        "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
        "\n",
        "# --- Create a thread ---\n",
        "thread = await client.threads.create()\n",
        "\n",
        "# --- Offline Stream Execution ---\n",
        "async for chunk in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    assistant_id=\"mock_agent\",\n",
        "    input=initial_input,\n",
        "    stream_mode=\"updates\",\n",
        "):\n",
        "    if chunk.data:\n",
        "        assistant_node = chunk.data.get(\"assistant\", {}).get(\"messages\", [])\n",
        "        tool_node = chunk.data.get(\"tools\", {}).get(\"messages\", [])\n",
        "\n",
        "        if assistant_node:\n",
        "            print(\"-\" * 20 + \" Assistant Node \" + \"-\" * 20)\n",
        "            print(assistant_node[-1])\n",
        "        elif tool_node:\n",
        "            print(\"-\" * 20 + \" Tools Node \" + \"-\" * 20)\n",
        "            print(tool_node[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "3dbc8795-c3f5-4559-a00e-dc410c0a927f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dbc8795-c3f5-4559-a00e-dc410c0a927f",
        "outputId": "9837936c-f80b-4e2c-e5f4-668d41776c58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧩 Current Thread ID: 408c05d4-2440-4832-9e8e-929af5bb9927\n",
            "📜 Messages so far:\n",
            "HUMAN: Multiply 2 and 3\n",
            "AI: The result is 6.\n"
          ]
        }
      ],
      "source": [
        "# --- Offline Thread Simulation for State Inspection ---\n",
        "import uuid\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Simulate thread creation (no API key required)\n",
        "thread = {\"thread_id\": str(uuid.uuid4()), \"messages\": []}\n",
        "\n",
        "# Simulate adding messages (like invoking graph)\n",
        "thread[\"messages\"].append(HumanMessage(content=\"Multiply 2 and 3\"))\n",
        "thread[\"messages\"].append(AIMessage(content=\"The result is 6.\"))\n",
        "\n",
        "# --- Tweak: Instead of API-based retrieval, mimic state inspection ---\n",
        "def get_thread_state(thread):\n",
        "    print(\"🧩 Current Thread ID:\", thread[\"thread_id\"])\n",
        "    print(\"📜 Messages so far:\")\n",
        "    for msg in thread[\"messages\"]:\n",
        "        print(f\"{msg.type.upper()}: {msg.content}\")\n",
        "\n",
        "# Run it\n",
        "get_thread_state(thread)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "11e6cde1-0388-43ea-b994-1c4e9ca1199b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11e6cde1-0388-43ea-b994-1c4e9ca1199b",
        "outputId": "9ca91ea2-72e5-4dbc-be8b-4200075cc54f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Message ID from Forked State: 1c3db6cb-4be9-4d8e-b615-5982c7a0df34\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "import uuid\n",
        "\n",
        "# --- Step 1: Simulate a stored thread state history ---\n",
        "all_states = [\n",
        "    {\n",
        "        \"checkpoint_id\": str(uuid.uuid4()),\n",
        "        \"values\": {\n",
        "            \"messages\": [\n",
        "                {\"id\": str(uuid.uuid4()), \"type\": \"human\", \"content\": \"Multiply 2 and 3\"},\n",
        "                {\"id\": str(uuid.uuid4()), \"type\": \"ai\", \"content\": \"The result is 6.\"}\n",
        "            ]\n",
        "        },\n",
        "        \"config\": {\"thread_id\": \"1\"}\n",
        "    },\n",
        "    {\n",
        "        \"checkpoint_id\": str(uuid.uuid4()),\n",
        "        \"values\": {\n",
        "            \"messages\": [\n",
        "                {\"id\": str(uuid.uuid4()), \"type\": \"human\", \"content\": \"Now divide 10 by 2\"},\n",
        "                {\"id\": str(uuid.uuid4()), \"type\": \"ai\", \"content\": \"The result is 5.\"}\n",
        "            ]\n",
        "        },\n",
        "        \"config\": {\"thread_id\": \"1\"}\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Step 2: Select a previous checkpoint for forking ---\n",
        "to_fork = all_states[-2]  # Picking the earlier one (Multiply 2 and 3)\n",
        "\n",
        "# --- Step 3: Access the first message ID safely ---\n",
        "first_message_id = to_fork['values']['messages'][0]['id']\n",
        "\n",
        "print(\"Selected Message ID from Forked State:\", first_message_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "0c1e2300-c8b2-4994-a96d-1be19c04b6a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c1e2300-c8b2-4994-a96d-1be19c04b6a8",
        "outputId": "61ab3442-f1a4-4953-bb8b-e12b771ac030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next node after this checkpoint: tools\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "import uuid\n",
        "\n",
        "# --- Step 1: Simulate stored state snapshots ---\n",
        "all_states = [\n",
        "    {\n",
        "        \"checkpoint_id\": str(uuid.uuid4()),\n",
        "        \"values\": {\n",
        "            \"messages\": [\n",
        "                {\"id\": str(uuid.uuid4()), \"type\": \"human\", \"content\": \"Multiply 2 and 3\"},\n",
        "                {\"id\": str(uuid.uuid4()), \"type\": \"ai\", \"content\": \"The result is 6.\"}\n",
        "            ]\n",
        "        },\n",
        "        \"config\": {\"thread_id\": \"1\"},\n",
        "        \"next\": \"tools\"  # <-- Simulated next node (technical tweak)\n",
        "    },\n",
        "    {\n",
        "        \"checkpoint_id\": str(uuid.uuid4()),\n",
        "        \"values\": {\n",
        "            \"messages\": [\n",
        "                {\"id\": str(uuid.uuid4()), \"type\": \"human\", \"content\": \"Now divide 10 by 2\"},\n",
        "                {\"id\": str(uuid.uuid4()), \"type\": \"ai\", \"content\": \"The result is 5.\"}\n",
        "            ]\n",
        "        },\n",
        "        \"config\": {\"thread_id\": \"1\"},\n",
        "        \"next\": \"assistant\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Step 2: Select a checkpoint to inspect ---\n",
        "to_fork = all_states[-2]  # selecting the earlier checkpoint\n",
        "\n",
        "# --- Step 3: Access the next node ---\n",
        "next_node = to_fork[\"next\"]\n",
        "\n",
        "print(\"Next node after this checkpoint:\", next_node)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "9d31d5aa-524f-42f4-ba7e-713a029610d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9d31d5aa-524f-42f4-ba7e-713a029610d6",
        "outputId": "e856c70d-fa1b-4f21-a46a-dc3dbca43d00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e03037c3-fd57-484d-b907-c11de83bce56'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "to_fork['checkpoint_id']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f11e1d9-9fe7-4243-a06f-9b07e38a12ad",
      "metadata": {
        "id": "8f11e1d9-9fe7-4243-a06f-9b07e38a12ad"
      },
      "source": [
        "Let's edit the state.\n",
        "\n",
        "Remember how our reducer on `messages` works:\n",
        "\n",
        "* It will append, unless we supply a message ID.\n",
        "* We supply the message ID to overwrite the message, rather than appending to state!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "0198f1b8-2f57-4c6e-ac6a-c6fb80cce0bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0198f1b8-2f57-4c6e-ac6a-c6fb80cce0bd",
        "outputId": "f1639041-7b43-4108-a942-fd9290275352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fork created successfully!\n",
            "\n",
            "Forked Config:\n",
            "{'thread_id': '1', 'checkpoint_id': '6a698f4b-43f8-44fd-bb54-3b8bf677618d', 'parent_checkpoint': 'f665f33a-a606-4085-a019-9bd132d2c5ed', 'input': {'messages': [HumanMessage(content='Multiply 3 and 3', additional_kwargs={}, response_metadata={}, id='84361a7d-6133-4126-b09b-cd842b46c12a')]}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "import uuid\n",
        "\n",
        "# --- Simulate a previous checkpoint ---\n",
        "to_fork = {\n",
        "    \"checkpoint_id\": str(uuid.uuid4()),\n",
        "    \"values\": {\n",
        "        \"messages\": [\n",
        "            {\"id\": str(uuid.uuid4()), \"type\": \"human\", \"content\": \"Multiply 2 and 3\"},\n",
        "            {\"id\": str(uuid.uuid4()), \"type\": \"ai\", \"content\": \"The result is 6.\"}\n",
        "        ]\n",
        "    },\n",
        "    \"config\": {\"thread_id\": \"1\"},\n",
        "}\n",
        "\n",
        "# --- Forking tweak: modify past message while retaining structure ---\n",
        "forked_input = {\n",
        "    \"messages\": [\n",
        "        HumanMessage(\n",
        "            content=\"Multiply 3 and 3\",\n",
        "            id=to_fork[\"values\"][\"messages\"][0][\"id\"]  # keeps same message ID (time-travel edit)\n",
        "        )\n",
        "    ]\n",
        "}\n",
        "\n",
        "# --- Simulated forked config update (no API, pure logic) ---\n",
        "forked_config = {\n",
        "    \"thread_id\": to_fork[\"config\"][\"thread_id\"],\n",
        "    \"checkpoint_id\": str(uuid.uuid4()),  # new checkpoint created for the fork\n",
        "    \"parent_checkpoint\": to_fork[\"checkpoint_id\"],\n",
        "    \"input\": forked_input\n",
        "}\n",
        "\n",
        "print(\" Fork created successfully!\\n\")\n",
        "print(\"Forked Config:\")\n",
        "print(forked_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "1dcd5b8e-6bb1-4967-84cf-4af710b8bf46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dcd5b8e-6bb1-4967-84cf-4af710b8bf46",
        "outputId": "ce561b7d-c055-462b-a315-ba7c3ffff556"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'thread_id': '1',\n",
              " 'checkpoint_id': '6a698f4b-43f8-44fd-bb54-3b8bf677618d',\n",
              " 'parent_checkpoint': 'f665f33a-a606-4085-a019-9bd132d2c5ed',\n",
              " 'input': {'messages': [HumanMessage(content='Multiply 3 and 3', additional_kwargs={}, response_metadata={}, id='84361a7d-6133-4126-b09b-cd842b46c12a')]}}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "forked_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "015ac68a-5cc1-4c42-90a2-5b2b4865a153",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "015ac68a-5cc1-4c42-90a2-5b2b4865a153",
        "outputId": "3d7627f3-9ef7-4338-ae47-6e4c2e175028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First stored checkpoint (states[0]):\n",
            "\n",
            "{'checkpoint_id': '1e9d6bc6-b487-4422-a04c-32ec99b60b73', 'node': 'assistant', 'values': {'messages': [{'type': 'human', 'content': 'Multiply 2 and 3'}, {'type': 'ai', 'content': 'The result is 6.'}]}, 'config': {'thread_id': '1'}}\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "\n",
        "# --- Step 1: Simulate the thread state history ---\n",
        "states = [\n",
        "    {\n",
        "        \"checkpoint_id\": str(uuid.uuid4()),\n",
        "        \"node\": \"assistant\",\n",
        "        \"values\": {\n",
        "            \"messages\": [\n",
        "                {\"type\": \"human\", \"content\": \"Multiply 2 and 3\"},\n",
        "                {\"type\": \"ai\", \"content\": \"The result is 6.\"}\n",
        "            ]\n",
        "        },\n",
        "        \"config\": {\"thread_id\": \"1\"}\n",
        "    },\n",
        "    {\n",
        "        \"checkpoint_id\": str(uuid.uuid4()),\n",
        "        \"node\": \"tools\",\n",
        "        \"values\": {\n",
        "            \"messages\": [\n",
        "                {\"type\": \"human\", \"content\": \"Now divide 10 by 2\"},\n",
        "                {\"type\": \"ai\", \"content\": \"The result is 5.\"}\n",
        "            ]\n",
        "        },\n",
        "        \"config\": {\"thread_id\": \"1\"}\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Step 2: Access the first stored state (like states[0]) ---\n",
        "print(\"First stored checkpoint (states[0]):\\n\")\n",
        "print(states[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3de80029-b987-49c5-890d-6cd70cbc8de7",
      "metadata": {
        "id": "3de80029-b987-49c5-890d-6cd70cbc8de7"
      },
      "source": [
        "To rerun, we pass in the `checkpoint_id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "da005240-d3f0-4c89-9aca-dfcb5d410ceb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da005240-d3f0-4c89-9aca-dfcb5d410ceb",
        "outputId": "36d36def-9166-47e4-f8e1-6170ec5b87b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Replaying from checkpoint: 6a698f4b-43f8-44fd-bb54-3b8bf677618d\n",
            "\n",
            "Receiving new event of type: assistant_node_start\n",
            "---------- Assistant Node ----------\n",
            "Assistant processing: Multiply 3 and 3\n",
            "\n",
            "Receiving new event of type: tool_node_execution\n",
            "---------- Tools Node ----------\n",
            "Tool executed: product_calc(3, 3) = 9\n",
            "\n",
            "Receiving new event of type: assistant_node_end\n",
            "---------- Assistant Node ----------\n",
            "Final Answer: The result is 9.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Pretend we are resuming from a previously forked checkpoint\n",
        "print(f\"\\nReplaying from checkpoint: {forked_config['checkpoint_id']}\\n\")\n",
        "\n",
        "# Mock events that would normally stream from the graph\n",
        "simulated_events = [\n",
        "    {\"event\": \"assistant_node_start\", \"data\": {\"messages\": [\"Assistant processing: Multiply 3 and 3\"]}},\n",
        "    {\"event\": \"tool_node_execution\", \"data\": {\"messages\": [\"Tool executed: product_calc(3, 3) = 9\"]}},\n",
        "    {\"event\": \"assistant_node_end\", \"data\": {\"messages\": [\"Final Answer: The result is 9.\"]}}\n",
        "]\n",
        "\n",
        "# Simulate live stream updates\n",
        "for chunk in simulated_events:\n",
        "    print(f\"Receiving new event of type: {chunk['event']}\")\n",
        "    if \"assistant\" in chunk[\"event\"]:\n",
        "        print(\"-\" * 10, \"Assistant Node\", \"-\" * 10)\n",
        "        print(chunk[\"data\"][\"messages\"][-1])\n",
        "    elif \"tool\" in chunk[\"event\"]:\n",
        "        print(\"-\" * 10, \"Tools Node\", \"-\" * 10)\n",
        "        print(chunk[\"data\"][\"messages\"][-1])\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36956571-a2b8-4f1b-8e30-51f02f155a6f",
      "metadata": {
        "id": "36956571-a2b8-4f1b-8e30-51f02f155a6f"
      },
      "source": [
        "### LangGraph Studio\n",
        "\n",
        "Let's look at forking in the Studio UI with our `agent`, which uses `module-1/studio/agent.py` set in `module-1/studio/langgraph.json`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}