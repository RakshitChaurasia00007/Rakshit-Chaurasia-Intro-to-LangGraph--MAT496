{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "83fcadf3",
      "metadata": {
        "id": "83fcadf3"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-2/chatbot-summarization.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239436-lesson-5-chatbot-w-summarizing-messages-and-memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b651ead9-5504-45ee-938d-f91ac78dddd1",
      "metadata": {
        "id": "b651ead9-5504-45ee-938d-f91ac78dddd1"
      },
      "source": [
        "# Chatbot with message summarization\n",
        "\n",
        "## Review\n",
        "\n",
        "We've covered how to customize graph state schema and reducer.\n",
        "\n",
        "We've also shown a number of ways to trim or filter messages in graph state.\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, let's take it one step further!\n",
        "\n",
        "Rather than just trimming or filtering messages, we'll show how to use LLMs to produce a running summary of the conversation.\n",
        "\n",
        "This allows us to retain a compressed representation of the full conversation, rather than just removing it with trimming or filtering.\n",
        "\n",
        "We'll incorporate this summarization into a simple Chatbot.  \n",
        "\n",
        "And we'll equip that Chatbot with memory, supporting long-running conversations without incurring high token cost / latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "000a6daa-92ad-4e57-a060-d1c81176eb0d",
      "metadata": {
        "id": "000a6daa-92ad-4e57-a060-d1c81176eb0d"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_core langgraph langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09201a62",
      "metadata": {
        "id": "09201a62"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"api key to be used here\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfddfce9-3a9b-4b35-a76d-28265515aabd",
      "metadata": {
        "id": "dfddfce9-3a9b-4b35-a76d-28265515aabd"
      },
      "source": [
        "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "464856d4",
      "metadata": {
        "id": "464856d4"
      },
      "outputs": [],
      "source": [
        "_set_env(\"lang key to be used here\")\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "537ade30-6a0e-4b6b-8bcd-ce90790b6392",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "537ade30-6a0e-4b6b-8bcd-ce90790b6392",
        "outputId": "30d95949-511a-4fbe-df0b-6683356232e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chat model initialized successfully with persistent summarization support.\n"
          ]
        }
      ],
      "source": [
        "# Import required modules\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os, getpass\n",
        "\n",
        "# --- Secure API Key Setup ---\n",
        "# Ask user for the API key only if it's not already set in the environment\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"api key to be used here\")\n",
        "\n",
        "# --- Model Initialization ---\n",
        "# Using a lighter model version for faster response and reduced token cost\n",
        "chat_model = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",    # optimized for low latency and summarization tasks\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(\"✅ Chat model initialized successfully with persistent summarization support.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db3afac3-8b7a-45db-a3c1-7e4125c1bc8b",
      "metadata": {
        "id": "db3afac3-8b7a-45db-a3c1-7e4125c1bc8b"
      },
      "source": [
        "We'll use `MessagesState`, as before.\n",
        "\n",
        "In addition to the built-in `messages` key, we'll now include a custom key (`summary`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "948e60f0-5c76-4235-b40e-cf523205d40e",
      "metadata": {
        "id": "948e60f0-5c76-4235-b40e-cf523205d40e"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState\n",
        "from datetime import datetime\n",
        "\n",
        "# Extended state schema with metadata for better memory tracking\n",
        "class State(MessagesState):\n",
        "    conversation_summary: str = \"\"    # stores the running summary\n",
        "    last_updated: str = \"\"            # timestamp of the last summary update\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6855ea31-5cc1-4277-a189-0b72459f67ec",
      "metadata": {
        "id": "6855ea31-5cc1-4277-a189-0b72459f67ec"
      },
      "source": [
        "We'll define a node to call our LLM that incorporates a summary, if it exists, into the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3f7d19b-afe0-4381-9b1a-0a832b162e7b",
      "metadata": {
        "id": "c3f7d19b-afe0-4381-9b1a-0a832b162e7b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "\n",
        "# Define the logic to call the model\n",
        "def call_model(state: State):\n",
        "\n",
        "    # Get recap if it exists\n",
        "    context_recap = state.get(\"context_recap\", \"\")\n",
        "\n",
        "    # If there is a recap, include it to help the model recall prior context\n",
        "    if context_recap:\n",
        "        system_message = f\"Brief recap of earlier chat: {context_recap}\"\n",
        "        # Combine recap with new user messages\n",
        "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
        "    else:\n",
        "        messages = state[\"messages\"]\n",
        "\n",
        "    # Call the model with updated context\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": response}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6882042c-b42d-4d52-a6a7-6ec8efa72450",
      "metadata": {
        "id": "6882042c-b42d-4d52-a6a7-6ec8efa72450"
      },
      "source": [
        "We'll define a node to produce a summary.\n",
        "\n",
        "Note, here we'll use `RemoveMessage` to filter our state after we've produced the summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78c7aa59-3760-4e76-93f1-bc713e3ec39e",
      "metadata": {
        "id": "78c7aa59-3760-4e76-93f1-bc713e3ec39e"
      },
      "outputs": [],
      "source": [
        "def summarize_conversation(state: State):\n",
        "\n",
        "    # Retrieve any existing recap\n",
        "    context_recap = state.get(\"context_recap\", \"\")\n",
        "\n",
        "    # Create our recap prompt\n",
        "    if context_recap:\n",
        "        recap_prompt = (\n",
        "            f\"Here is the previous context recap:\\n{context_recap}\\n\\n\"\n",
        "            \"Now update it by adding key points from the latest conversation above. \"\n",
        "            \"Keep it concise, in short bullet points.\"\n",
        "        )\n",
        "    else:\n",
        "        recap_prompt = \"Write a short context recap in bullet points for the conversation above.\"\n",
        "\n",
        "    # Add recap prompt to history\n",
        "    messages = state[\"messages\"] + [HumanMessage(content=recap_prompt)]\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # Keep only the 3 most recent messages (slightly different from original)\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-3]]\n",
        "\n",
        "    return {\"context_recap\": response.content, \"messages\": delete_messages}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f982993e-f4be-4ff7-9a38-886f75398b3d",
      "metadata": {
        "id": "f982993e-f4be-4ff7-9a38-886f75398b3d"
      },
      "source": [
        "We'll add a conditional edge to determine whether to produce a summary based on the conversation length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b507665d-7f5d-442a-b498-218c94c5dd8b",
      "metadata": {
        "id": "b507665d-7f5d-442a-b498-218c94c5dd8b"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END\n",
        "from typing_extensions import Literal\n",
        "\n",
        "# Determine whether to end or generate a context recap\n",
        "def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
        "    \"\"\"Decide the next step based on conversation length and message size.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # Trigger recap if conversation is long or a message is too large\n",
        "    total_chars = sum(len(m.content) for m in messages if hasattr(m, \"content\"))\n",
        "\n",
        "    # Recap if more than 6 messages OR total text exceeds 800 characters\n",
        "    if len(messages) > 6 or total_chars > 800:\n",
        "        return \"summarize_conversation\"\n",
        "\n",
        "    # Otherwise, end the flow\n",
        "    return END\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a838f4c-7067-4f7f-a4c4-6654e11214cd",
      "metadata": {
        "id": "5a838f4c-7067-4f7f-a4c4-6654e11214cd"
      },
      "source": [
        "## Adding memory\n",
        "\n",
        "Recall that [state is transient](https://github.com/langchain-ai/langgraph/discussions/352#discussioncomment-9291220) to a single graph execution.\n",
        "\n",
        "This limits our ability to have multi-turn conversations with interruptions.\n",
        "\n",
        "As introduced at the end of Module 1, we can use [persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) to address this!\n",
        "\n",
        "LangGraph can use a checkpointer to automatically save the graph state after each step.\n",
        "\n",
        "This built-in persistence layer gives us memory, allowing LangGraph to pick up from the last state update.\n",
        "\n",
        "As we previously showed, one of the easiest to work with is `MemorySaver`, an in-memory key-value store for Graph state.\n",
        "\n",
        "All we need to do is compile the graph with a checkpointer, and our graph has memory!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d57516d-f9f1-4d3c-a84a-7277b5ce6df6",
      "metadata": {
        "id": "1d57516d-f9f1-4d3c-a84a-7277b5ce6df6"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END\n",
        "from typing_extensions import Literal\n",
        "\n",
        "# Determine whether to end or generate a recap\n",
        "def should_continue(state: State) -> Literal[\"generate_recap\", END]:\n",
        "    \"\"\"Decide the next step based on conversation length and message size.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    total_chars = sum(len(m.content) for m in messages if hasattr(m, \"content\"))\n",
        "\n",
        "    # Trigger recap if conversation is long or text exceeds threshold\n",
        "    if len(messages) > 6 or total_chars > 800:\n",
        "        return \"generate_recap\"\n",
        "\n",
        "    # Otherwise, end the flow\n",
        "    return END\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0bd5d23-ac3b-4496-a049-9a9f97d2feb9",
      "metadata": {
        "id": "d0bd5d23-ac3b-4496-a049-9a9f97d2feb9"
      },
      "source": [
        "## Threads\n",
        "\n",
        "The checkpointer saves the state at each step as a checkpoint.\n",
        "\n",
        "These saved checkpoints can be grouped into a `thread` of conversation.\n",
        "\n",
        "Think about Slack as an analog: different channels carry different conversations.\n",
        "\n",
        "Threads are like Slack channels, capturing grouped collections of state (e.g., conversation).\n",
        "\n",
        "Below, we use `configurable` to set a thread ID.\n",
        "\n",
        "![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbadf3b379c2ee621adfd1_chatbot-summarization1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2566c93b-13e6-4a53-bc0f-b00fff691d30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2566c93b-13e6-4a53-bc0f-b00fff691d30",
        "outputId": "3348654b-3774-489a-9cde-4c9016f7807c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "(Mock response) Hi! I'm Lance\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "(Mock response) What's my name?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "(Mock response) I like the 49ers!\n",
            "\n",
            "--- Current Context Recap ---\n",
            "No recap yet.\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# 1️⃣ Imports\n",
        "# ---------------------------\n",
        "from langgraph.graph import MessagesState, StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "\n",
        "# ---------------------------\n",
        "# 2️⃣ Mock LLM to avoid API errors\n",
        "# ---------------------------\n",
        "class MockModel:\n",
        "    \"\"\"Fake LLM model to simulate responses without API key\"\"\"\n",
        "    def invoke(self, messages):\n",
        "        # Last user message\n",
        "        user_msg = messages[-1].content if messages else \"\"\n",
        "        # Build a simple mock response\n",
        "        return [HumanMessage(content=f\"(Mock response) {user_msg}\")]\n",
        "\n",
        "model = MockModel()  # Use mock model — safe, no API calls\n",
        "\n",
        "# ---------------------------\n",
        "# 3️⃣ Define State\n",
        "# ---------------------------\n",
        "class State(MessagesState):\n",
        "    context_recap: str  # Stores bullet-style recap\n",
        "\n",
        "# ---------------------------\n",
        "# 4️⃣ Define Nodes\n",
        "# ---------------------------\n",
        "def call_model(state: State):\n",
        "    context_recap = state.get(\"context_recap\", \"\")\n",
        "    if context_recap:\n",
        "        system_msg = f\"Brief recap of earlier chat: {context_recap}\"\n",
        "        messages = [SystemMessage(content=system_msg)] + state[\"messages\"]\n",
        "    else:\n",
        "        messages = state[\"messages\"]\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def summarize_conversation(state: State):\n",
        "    context_recap = state.get(\"context_recap\", \"\")\n",
        "    if context_recap:\n",
        "        recap_prompt = f\"Previous recap:\\n{context_recap}\\n\\nUpdate it with latest conversation in bullet points.\"\n",
        "    else:\n",
        "        recap_prompt = \"Write a short context recap in bullet points for the conversation above.\"\n",
        "    messages = state[\"messages\"] + [HumanMessage(content=recap_prompt)]\n",
        "    response = model.invoke(messages)\n",
        "    # Keep last 3 messages for memory\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-3]]\n",
        "    return {\"context_recap\": response[0].content, \"messages\": delete_messages}\n",
        "\n",
        "def should_continue(state: State):\n",
        "    messages = state[\"messages\"]\n",
        "    total_chars = sum(len(m.content) for m in messages if hasattr(m, \"content\"))\n",
        "    # Trigger recap if conversation is long\n",
        "    if len(messages) > 6 or total_chars > 800:\n",
        "        return \"generate_recap\"\n",
        "    return END\n",
        "\n",
        "# ---------------------------\n",
        "# 5️⃣ Build Graph\n",
        "# ---------------------------\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(\"chat_flow\", call_model)\n",
        "workflow.add_node(\"generate_recap\", summarize_conversation)\n",
        "workflow.add_edge(START, \"chat_flow\")\n",
        "workflow.add_conditional_edges(\"chat_flow\", should_continue)\n",
        "workflow.add_edge(\"generate_recap\", END)\n",
        "\n",
        "memory = MemorySaver()\n",
        "graph = workflow.compile(checkpointer=memory)\n",
        "\n",
        "# ---------------------------\n",
        "# 6️⃣ Conversation Thread\n",
        "# ---------------------------\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "messages_to_send = [\n",
        "    \"Hi! I'm Lance\",\n",
        "    \"What's my name?\",\n",
        "    \"I like the 49ers!\"\n",
        "]\n",
        "\n",
        "for msg in messages_to_send:\n",
        "    input_message = HumanMessage(content=msg)\n",
        "    output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "    for m in output['messages'][-1:]:\n",
        "        m.pretty_print()\n",
        "\n",
        "# ---------------------------\n",
        "# 7️⃣ Show Context Recap\n",
        "# ---------------------------\n",
        "print(\"\\n--- Current Context Recap ---\")\n",
        "print(graph.get_state(config).values.get(\"context_recap\", \"No recap yet.\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "531e5b63-5e8b-486e-baa0-a45521e2fbc2",
      "metadata": {
        "id": "531e5b63-5e8b-486e-baa0-a45521e2fbc2"
      },
      "source": [
        "Now, we don't yet have a summary of the state because we still have < = 6 messages.\n",
        "\n",
        "This was set in `should_continue`.\n",
        "\n",
        "```\n",
        "    # If there are more than six messages, then we summarize the conversation\n",
        "    if len(messages) > 6:\n",
        "        return \"summarize_conversation\"\n",
        "```\n",
        "\n",
        "We can pick up the conversation because we have the thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "91b82aaa-17f9-49e2-9528-f4b22e23ebcb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "91b82aaa-17f9-49e2-9528-f4b22e23ebcb",
        "outputId": "96374619-9b3f-457e-a81d-d9d4772e003d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(Mock response) Write a short context recap in bullet points for the conversation above.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "graph.get_state(config).values.get(\"context_recap\", \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "068a93e9-f716-4980-8edf-94115017d865",
      "metadata": {
        "id": "068a93e9-f716-4980-8edf-94115017d865"
      },
      "source": [
        "The `config` with thread ID allows us to proceed from the previously logged state!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "24b34f0f-62ef-4008-8e96-480cbe92ea3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24b34f0f-62ef-4008-8e96-480cbe92ea3e",
        "outputId": "eccb263f-5564-4ab4-d084-9b136fc75dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Responses ---\n",
            "Response 1:\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "(Mock response) I like Nick Bosa! Is he currently the highest-paid defensive player?\n",
            "\n",
            "--- Updated Context Recap (Bullet Points) ---\n",
            "(Mock response) Previous recap:\n",
            "(Mock response) Write a short context recap in bullet points for the conversation above.\n",
            "\n",
            "Update it with latest conversation in bullet points.\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# 1️⃣ Send a new message to the graph\n",
        "# ---------------------------\n",
        "new_message = \"I like Nick Bosa! Is he currently the highest-paid defensive player?\"\n",
        "input_message = HumanMessage(content=new_message)\n",
        "\n",
        "# Invoke the graph and get output\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "\n",
        "# ---------------------------\n",
        "# 2️⃣ Print all new responses neatly\n",
        "# ---------------------------\n",
        "print(\"\\n--- Model Responses ---\")\n",
        "for idx, msg in enumerate(output['messages'][-1:], start=1):\n",
        "    print(f\"Response {idx}:\")\n",
        "    msg.pretty_print()\n",
        "\n",
        "# ---------------------------\n",
        "# 3️⃣ Display updated context recap in a clean format\n",
        "# ---------------------------\n",
        "recap = graph.get_state(config).values.get(\"context_recap\", \"No recap yet.\")\n",
        "print(\"\\n--- Updated Context Recap (Bullet Points) ---\")\n",
        "print(recap)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "22f1b35f-e4bb-47f6-87b1-d84d8aed9aa9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "22f1b35f-e4bb-47f6-87b1-d84d8aed9aa9",
        "outputId": "94f78516-ace9-4019-b30d-36ef51307531"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(Mock response) Previous recap:\\n(Mock response) Write a short context recap in bullet points for the conversation above.\\n\\nUpdate it with latest conversation in bullet points.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "graph.get_state(config).values.get(\"context_recap\", \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad7cc0ab-905a-4037-b7cb-69db5b89591e",
      "metadata": {
        "id": "ad7cc0ab-905a-4037-b7cb-69db5b89591e"
      },
      "source": [
        "## LangSmith\n",
        "\n",
        "Let's review the trace!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}